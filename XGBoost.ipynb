{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Machine Learning library: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Some recap\n",
    "\n",
    "\n",
    "Supervised learning problem: classification, regression, ranking, recommendation.     \n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XGBoost Introduction\n",
    "\n",
    "Optimised gradient-boosting machine learning library.      \n",
    "Has API in several language: Python, R etc.       \n",
    "Speed and performance is its key.       \n",
    "Core XGBoost algorithm is parallelisable, it can harness all of the processing power of modern multi-core computers.     \n",
    "Consistently outperforms single-algorithm methods in ML competition and has been shown to achieve state of the art performance on a variety of benchmark ML dataset.      \n",
    "\n",
    "When to use XGBoost?     \n",
    "\n",
    "Supervised ML task that fits:     \n",
    "1) large number of training example (more than 1000 traning sample and less than 100 features)     \n",
    "2) number of feature < number of training samples      \n",
    "3) mixture of categorical and numerical features, or just numerical feature.     \n",
    "\n",
    "When NOT to use XGBoost?    \n",
    "\n",
    "1) Image recognition, computer vision or NLP (use Deep Learning instead)     \n",
    "2) small training set (<100)      \n",
    "3) training sample << no. of feature       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick example, DON'T RUN\n",
    "\n",
    "class_data = pd.read_csv(\"classification_data.csv\")\n",
    "X,y = class_data.iloc[:,:-1], class_data.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=123)\n",
    "\n",
    "#instantiate xgboost\n",
    "xg_cl = xgb.XGBClassifier(objective=\"binary:logistic\", n_estimators=10,seed=123)\n",
    "\n",
    "xg_cl.fit(X_train,y_train)\n",
    "\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "#accuracy of the trained model\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost for Classification\n",
    "\n",
    "Classification problem:     \n",
    "\n",
    "When dealing with binary supervised learning problems, the AUC (Area under the Receiver Operating Characteristic (ROC)) is the most versatile and common evaluation metric used to judge the quality of a binary classification model. It is simply the probability that a randomly chosen positive data point will have a higher rank than a randomly chosen negative data point for the learning problem. Higher AUC = more sensitive, better performing model.      \n",
    "\n",
    "When dealing with multi-class classification problem, it is common to use accuracy score (higher is better) or to look at overall confusion matrix to evaluate the quality of a model.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Decision Tree (for classification problem here)\n",
    "\n",
    "XGBoost is usually used with trees as base learner.     \n",
    "At each node, a question will be asked.      \n",
    "At the bottom, every possible decision will eventualy lead to a choice, some taking many fewer questions to get to those choice than others.      \n",
    "\n",
    "Base learner: Any individual learning algorithm in an ensemble algorithm as a base learner.      \n",
    "\n",
    "Decision trees are contructed iteratively (one decision at a time),     \n",
    "until a stopping criterion is met (e.g. depth of tree reaches some pre-defined value).     \n",
    "During construction, the tree is built one split at a time, and the way that a split is selected (that is, what feature to split on and the where in the feature's range of values to split) can vary, it involves a stategy that segregates the target values better. (put each target category into buckets that are increasingly dominated by just one category), until nearly all vaues within a given split are exclusively of one category or another.             \n",
    "\n",
    "Each leaf will have a single category in the majority, or should be exclusively of one category.     \n",
    "\n",
    "Individual decision trees in geneal are low-bias, high-variance learning model. (could be fairly accuracy, but not precise) i.e. they are good at learning relationships within any data we train them on, but they tend to overfit the data we use to train them on, and generalise on new data poorly.     \n",
    "\n",
    "XBGoost uses CART as base learner. In contrast, CART contain real-valued score in each leaf, regardless of whether they are used for classification or regression. The real-value scores can then be thresholded to convert into categories for classification problems if necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Boosting\n",
    "\n",
    "Boosting is not a specific ML algorithm, but a concept that can be applied to a set of machine leanring models (meta-algorithm).     \n",
    "\n",
    "Specifically, it is an ensemble meta-algorithm primarily used to reduce any given single learner's variance and to convert many weak learners into an arbitrarily strong learner.      \n",
    "\n",
    "Weak learner: any ML algorithm that is slighly better than chance.(e.g. dicision stump with depth = 1, a decision tree whose prediction are slighly better than 50%.).      \n",
    "\n",
    "Strong learner: any algorithm that can be tuned to achieve arbitrarily good performance for some supervised learning problem.     \n",
    "\n",
    "How boosting is accomplished:        \n",
    "1) iteratively learning a set of weak models on subsets of the data.     \n",
    "2) weighting each of their predictions according to each weak learner's performance.      \n",
    "3) Combine the weighted predictions to obtain a single weighted prediction.      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DMatrix (and CV iXGBoost)\n",
    "\n",
    "In XGBoost, the dataset is convert into an optimised data structure (for performance and efficiency).      \n",
    "\n",
    "Normally, the input datasets will be converted into DMatrix on the fly.    \n",
    "\n",
    "but when we used the XGBoost cv object (for cross validation), we have to first explicityly convert our data into a Dmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T RUN\n",
    "# DMatrix and CV in XGBoost\n",
    "\n",
    "class_data = pd.read_csv(\"classification_data.csv\")\n",
    "# DMatrix convert\n",
    "# month_5_still_here is the binary target, last col in the dataset.\n",
    "churn_dmatrix = xgb.DMatrix(data=class_data.iloc[:,:-1],label=class_data.month_5_still_here)\n",
    "\n",
    "params = {\"objective\":\"binary:logistic\",\"max_depth\":4}\n",
    "\n",
    "# num_boost_round: how many trees we want to build\n",
    "# output to be sotres as pd df\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix,params=params, nfold=4, num_boost_round=10, metrics=\"error\",as_pandas=True)\n",
    "\n",
    "print(\"Accuracy: %f\" %((1-cv_results[\"test-error-mean\"]).iloc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost for Regression\n",
    "\n",
    "In most case, root mean squared error (RMSE) or the mean absolute error (MAE) is used to evaluate the quality of a regression model.    \n",
    "\n",
    "RMSE treats positive and negative error equally but punish larger differences between predicted and actual values much more than smaller ones.     \n",
    "\n",
    "MAE simply sums the absolute differences between predicted and actual values across all samples (then take the mean). MAE is not affected by large difference as much as RMSE, it lacks some nice maths property and it is much less used as an evaluation metrics.      \n",
    "\n",
    "Common regression algorithms: Linear regression, Decision trees (CART)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Objective (loss) functions and base learners\n",
    "\n",
    "Loss function quantify how far off a prediction is form the actual result for a given data point.     \n",
    "It maps the difference between estimated and true values for some collection of data.     \n",
    "Goal: find the model that yields the minimum value of the loss function.     \n",
    "\n",
    "\n",
    "Loss function has specific naming conventions in XGBoost:      \n",
    "For regression model:         \n",
    "reg:linear       \n",
    "\n",
    "For binary classification:      \n",
    "reg:logistic (when we want just decision, not probability)     \n",
    "binary:logistic (when we want probability rather than just decision)       \n",
    "\n",
    "XGBoost want base learner (i.e. individual model in the ensemble) when combined create final prediciton that is non-linear.    \n",
    "\n",
    "Each base learner should be good at distinguishing or predicting different parts of the dataset.      \n",
    "\n",
    "Two kinds of base learner: tree and linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree as base learner: scikit-learn API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
