{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Library: TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run this from tf env, with tensorflow 2.0+**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "\n",
    "Tensorflow is an open-source library for graph-based numerical computation.     \n",
    "Has both low and high level APIs.       \n",
    "Can be used to perform addition, multiplication and differentiation.      \n",
    "Can be used to train ML models.       \n",
    "\n",
    "Important changes in TensorFlow 2.0:        \n",
    "Eager exucution is now enabled y default, which allows users to write simpler and more intuitive code.       \n",
    "Modeling building is now centered around the Keras and Estimators high-level APIs.         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0D Tensor\n",
    "d0 = tf.ones((1,))\n",
    "d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8, shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1D Tensor\n",
    "d1 = tf.ones((2,))\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=11, shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1., 1.],\n",
       "       [1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2D Tensor\n",
    "d2 = tf.ones((2,2))\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=14, shape=(2, 2, 2), dtype=float32, numpy=\n",
       "array([[[1., 1.],\n",
       "        [1., 1.]],\n",
       "\n",
       "       [[1., 1.],\n",
       "        [1., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3D Tensor\n",
    "d3 = tf.ones((2,2,2))\n",
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 1.]\n",
      "  [1. 1.]]\n",
      "\n",
      " [[1. 1.]\n",
      "  [1. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "# Print the 3D tensor\n",
    "print(d3.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Defining constants in TensorFlow\n",
    "\n",
    "A constant is the simplest category of tensor.      \n",
    "A constant does not change and cannot be trained.      \n",
    "\n",
    "It can, however, have any dimension.      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import constant\n",
    "\n",
    "# Define a 2x3 constant tensor of 3s.\n",
    "a = constant(3, shape=[2,3])\n",
    "\n",
    "# define a 2x2 tensor, which is constructed from the 1D tensor: 1,2,3,4\n",
    "b = constant([1,2,3,4],shape=[2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 3 3]\n",
      " [3 3 3]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(a.numpy())\n",
    "print(b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "[[0 0]\n",
      " [0 0]]\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n",
      "[[1 1]\n",
      " [1 1]]\n",
      "[[7 7 7]\n",
      " [7 7 7]\n",
      " [7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "# some convenient way of defining constant\n",
    "input_tensor = constant([1,2,3,4],shape=[2,2])\n",
    "\n",
    "a = tf.constant([1,2,3])\n",
    "\n",
    "b = tf.zeros([2,2])\n",
    "\n",
    "c = tf.zeros_like(input_tensor)\n",
    "\n",
    "d = tf.ones([2,2])\n",
    "\n",
    "e = tf. ones_like(input_tensor)\n",
    "\n",
    "f = tf.fill([3,3],7)\n",
    "\n",
    "print(a.numpy())\n",
    "print(b.numpy())\n",
    "print(c.numpy())\n",
    "print(d.numpy())\n",
    "print(e.numpy())\n",
    "print(f.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Defining and initialising variables\n",
    "\n",
    "Unlike a constant, a variable's value can change during computation. The value of a variable is shared, persistent and modifiable.         \n",
    "Its data type and shape are fixed.         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variable\n",
    "a0 = tf.Variable([1,2,3,4,5,6], dtype=tf.float32)\n",
    "a1 = tf.Variable([1,2,3,4,5,6], dtype=tf.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a constant\n",
    "b = tf.constant(2,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute their product\n",
    "c0 = tf.multiply(a0,b)\n",
    "c1 = a0*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.  8. 10. 12.]\n",
      "[ 2.  4.  6.  8. 10. 12.]\n"
     ]
    }
   ],
   "source": [
    "print(c0.numpy())\n",
    "print(c1.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining data as constants:        \n",
    "\n",
    "After you have imported constant, you will use it to transform a numpy array, credit_numpy, into a tensorflow constant, credit_constant. This array contains feature columns from a dataset on credit card holders and is previewed in the image below. We will return to this dataset in later chapters.             \n",
    "\n",
    "Note that tensorflow version 2.0 allows you to use data as either a numpy array or a tensorflow constant object. Using a constant will ensure that any operations performed with that object are done in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_numpy = np.array([[ 2.0000e+00,  2.4000e+01,  1.0000e+00,  3.9130e+03],\n",
    "       [ 2.0000e+00,  2.6000e+01,  2.0000e+00,  2.6820e+03],\n",
    "       [ 2.0000e+00,  3.4000e+01,  2.0000e+00,  2.9239e+04],\n",
    "       [ 2.0000e+00,  3.7000e+01,  2.0000e+00,  3.5650e+03],\n",
    "       [ 3.0000e+00,  4.1000e+01,  1.0000e+00, -1.6450e+03],\n",
    "       [ 2.0000e+00,  4.6000e+01,  1.0000e+00,  4.7929e+04]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datatype is: <dtype: 'float64'>\n",
      "The shape is: (6, 4)\n"
     ]
    }
   ],
   "source": [
    "# Import constant from TensorFlow\n",
    "from tensorflow import constant\n",
    "\n",
    "# Convert the credit_numpy array into a tensorflow constant\n",
    "credit_constant = constant(credit_numpy)\n",
    "\n",
    "# Print constant datatype\n",
    "print('The datatype is:', credit_constant.dtype)\n",
    "\n",
    "# Print constant shape\n",
    "print('The shape is:', credit_constant.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Operation of Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow has a model of computation that revolves around the use of graphs. A Tensorflow graph contains edges and nodes, where the edges are tensors and the modes are operations.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The add() operation performs element-wise **addition** with two tensors.       \n",
    "Element-wise addition requires both tensors to have the same shape.            \n",
    "The add() operator is overloaded, which means that we can also perform addition using the plus symbol.      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[4 6]\n",
      "[[ 6  8]\n",
      " [10 12]]\n"
     ]
    }
   ],
   "source": [
    "# addition operator\n",
    "from tensorflow import constant, add\n",
    "\n",
    "# Define 0-dimensional tensors\n",
    "A0 = constant([1])\n",
    "B0 = constant([2])\n",
    "\n",
    "# Define 1-dimensional tensors\n",
    "A1 = constant([1,2])\n",
    "B1 = constant([3,4])\n",
    "\n",
    "# Define 2-dimensional tensors\n",
    "A2 = constant([[1,2],[3,4]])\n",
    "B2 = constant([[5,6],[7,8]])\n",
    "\n",
    "# Perform tensor addition with add()\n",
    "C0 = add(A0,B0) #scaler addition\n",
    "C1 = add(A1,B1) #vector addition\n",
    "C2 = add(A2,B2) #matrix addition\n",
    "\n",
    "print(C0.numpy())\n",
    "print(C1.numpy())\n",
    "print(C2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Element-wise multiplication** performed using multiply() operation.        \n",
    "The tensors involved must have the same shape.     \n",
    "**Matrix multiplication** performed wutg matmul() operator.         \n",
    "The matmul(A,B) operation multiplies A by B.       \n",
    "Number of columns of A must equal to the number of rows of B.      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import ones, matmul, multiply\n",
    "\n",
    "# Define tensors\n",
    "A0 = ones(1)\n",
    "A31 = ones([3,1])\n",
    "A34 = ones([3,4])\n",
    "A43 = ones([4,3])\n",
    "\n",
    "ex1 = multiply(A0,A0)\n",
    "ex2 = multiply(A31,A31)\n",
    "ex3 = multiply(A34,A34)\n",
    "\n",
    "print(ex1.numpy())\n",
    "print(ex2.numpy())\n",
    "print(ex3.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing over tensor dimensions:          \n",
    "\n",
    "The reduce_sum() operator sums over the dimensions of a tensor.      \n",
    "reduce_sum(A) sums over all dimensions of A.      \n",
    "reduce_sum(A,i) sums over dimension i.       \n",
    "In each case we reduce the size of the tensor by summing over one of its dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]]\n",
      "24.0\n",
      "[[2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]]\n",
      "[[3. 3. 3. 3.]\n",
      " [3. 3. 3. 3.]]\n",
      "[[4. 4. 4.]\n",
      " [4. 4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import ones, reduce_sum\n",
    "\n",
    "# Define a 2x3x4 tensors of ones\n",
    "A = ones([2,3,4])\n",
    "\n",
    "# Sum over all dimension\n",
    "B = reduce_sum(A)\n",
    "\n",
    "# Sum over dimension 0,1,2\n",
    "B0 = reduce_sum(A,0) # 3x4 matrix of 2s\n",
    "B1 = reduce_sum(A,1) # 2x4 matrix of 3s\n",
    "B2 = reduce_sum(A,2) # 2x3 matrix of 4s\n",
    "\n",
    "print(A.numpy())\n",
    "print(B.numpy())\n",
    "print(B0.numpy())\n",
    "print(B1.numpy())\n",
    "print(B2.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1: [1 2 3 4]\n",
      "C23: [[1 2 3]\n",
      " [1 6 4]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import ones_like\n",
    "\n",
    "# Define tensors A1 and A23 as constants\n",
    "A1 = constant([1, 2, 3, 4])\n",
    "A23 = constant([[1, 2, 3], [1, 6, 4]])\n",
    "\n",
    "# Define B1 and B23 to have the correct shape\n",
    "B1 = ones_like(A1)\n",
    "B23 = ones_like(A23)\n",
    "\n",
    "# Perform element-wise multiplication\n",
    "C1 = multiply(A1,B1)\n",
    "C23 = multiply(A23,B23)\n",
    "\n",
    "# Print the tensors C1 and C23\n",
    "print('C1: {}'.format(C1.numpy()))\n",
    "print('C23: {}'.format(C23.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1687]\n",
      " [-3218]\n",
      " [-1933]\n",
      " [57850]]\n"
     ]
    }
   ],
   "source": [
    "# Define features, params, and bill as constants\n",
    "features = constant([[2, 24], [2, 26], [2, 57], [1, 37]])\n",
    "params = constant([[1000], [150]])\n",
    "bill = constant([[3913], [2682], [8617], [64400]])\n",
    "\n",
    "# Compute billpred using features and params\n",
    "billpred = matmul(features,params)\n",
    "\n",
    "# Compute and print the error\n",
    "error = bill - billpred\n",
    "print(error.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11 50]\n",
      " [ 7  2]\n",
      " [ 4 60]\n",
      " [ 3  0]\n",
      " [25 10]]\n",
      "172\n",
      "[ 50 122]\n",
      "[61  9 64  3 35]\n"
     ]
    }
   ],
   "source": [
    "wealth = constant([[11, 50], [7, 2], [4, 60], [3, 0],[25,10]])\n",
    "\n",
    "wealth_all = reduce_sum(wealth)\n",
    "# Sum over dimension 0,1\n",
    "wealth0 = reduce_sum(wealth,0) # 3x4 matrix of 2s\n",
    "wealth1 = reduce_sum(wealth,1) # 2x4 matrix of 3s\n",
    "\n",
    "\n",
    "print(wealth.numpy())\n",
    "print(wealth_all.numpy())\n",
    "print(wealth0.numpy())\n",
    "print(wealth1.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient():         \n",
    "Computes the slope of a function at a point.       \n",
    "\n",
    "reshape():        \n",
    "Reshape a tensor (e.g. 10x10 to 100x1)         \n",
    "\n",
    "random():       \n",
    "Populates tensor with entries drawn.        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the optimum (gradient())**        \n",
    "\n",
    "In many ML problems, we will want to find the optimum of a funciton, e.g. minimise the loss function or maximise the objective function.        \n",
    "\n",
    "Minimum: lowest value of a loss function.          \n",
    "Maximum: highest value of objective function.           \n",
    "\n",
    "We can do this using the gradient() operation.      \n",
    "We start this process by passing points to the gradient operation until we find one where the gradient is 0, that is the **optimum**.        \n",
    "Minimum: change in gradient > 0.       \n",
    "Maximum: change in gradient < 0.          \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0\n"
     ]
    }
   ],
   "source": [
    "# Define x\n",
    "x = tf.Variable(-1.0)\n",
    "\n",
    "# Define y to be x^2 wihtin instance of GradientTape\n",
    "# watch method to an instance of gradient tape and then pass the variable x,\n",
    "# this will allow us to compute the rate of change of y wrt x.\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = tf.multiply(x,x)\n",
    "    \n",
    "# Evaluate the gradient of y at x = -1, using the tape instance of gradienttape\n",
    "g = tape.gradient(y,x)\n",
    "print(g.numpy())\n",
    "\n",
    "# slope is -2 at x = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Images as tensor (reshape())**        \n",
    "\n",
    "A grayscale image has a natural respresentation as matrix with values between 0 and 255. Some algorithm require us to reshape matrices into vectors before using them as inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[221]\n",
      " [232]\n",
      " [206]\n",
      " [111]]\n",
      "[[105 109  38]\n",
      " [145 170 187]\n",
      " [ 94 225  87]\n",
      " [ 50 136 170]]\n"
     ]
    }
   ],
   "source": [
    "# generate grayscale image\n",
    "gray = tf.random.uniform([2,2],maxval=255, dtype=\"int32\")\n",
    "\n",
    "# reshape grayscale image\n",
    "gray = tf.reshape(gray, [2*2,1])\n",
    "\n",
    "print(gray.numpy())\n",
    "# generate a color image\n",
    "color = tf.random.uniform([2,2,3],maxval=255, dtype=\"int32\")\n",
    "\n",
    "# reshape color image\n",
    "color = tf.reshape(color,[2*2,3])\n",
    "print(color.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a loss function, y=x2, which you want to minimize. You can do this by computing the slope using the GradientTape() operation at different values of x. If the slope is positive, you can decrease the loss by lowering x. If it is negative, you can decrease it by increasing x. This is how gradient descent works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0\n",
      "2.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def compute_gradient(x0):\n",
    "  \t# Define x as a variable with an initial value of x0\n",
    "\tx = tf.Variable(x0)\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\ttape.watch(x)\n",
    "        # Define y using the multiply operation\n",
    "\t\ty = tf.multiply(x,x)\n",
    "    # Return the gradient of y with respect to x\n",
    "\treturn tape.gradient(y, x).numpy()\n",
    "\n",
    "# Compute and print gradients at x = -1, 1, and 0\n",
    "print(compute_gradient(-1.0))\n",
    "print(compute_gradient(1.0))\n",
    "print(compute_gradient(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Input data\n",
    "\n",
    "When we train a ML model, we will want to import data from an external source. Numeric data wil need to be assigned a type, and text and image data will need to be converted into a usable format.       \n",
    "\n",
    "External datasets can be imported using TensorFlow. It is useful for complex data pipeline. We can also use pandas to import data. And then we can convert data into numpy array, which we can use without further modification in TensorFlow.     \n",
    "Dataset will contain columns with different data types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0           0     0  ...      7        1180              0   \n",
       "1      7242     2.0           0     0  ...      7        2170            400   \n",
       "2     10000     1.0           0     0  ...      6         770              0   \n",
       "3      5000     1.0           0     0  ...      7        1050            910   \n",
       "4      8080     1.0           0     0  ...      8        1680              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "3      1965             0    98136  47.5208 -122.393           1360   \n",
       "4      1987             0    98074  47.6168 -122.045           1800   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "1        7639  \n",
       "2        8062  \n",
       "3        5000  \n",
       "4        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = pd.read_csv(\"../Machine_Learning_basics/data/kc_house_data.csv\")\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21613 entries, 0 to 21612\n",
      "Data columns (total 21 columns):\n",
      "id               21613 non-null int64\n",
      "date             21613 non-null object\n",
      "price            21613 non-null float64\n",
      "bedrooms         21613 non-null int64\n",
      "bathrooms        21613 non-null float64\n",
      "sqft_living      21613 non-null int64\n",
      "sqft_lot         21613 non-null int64\n",
      "floors           21613 non-null float64\n",
      "waterfront       21613 non-null int64\n",
      "view             21613 non-null int64\n",
      "condition        21613 non-null int64\n",
      "grade            21613 non-null int64\n",
      "sqft_above       21613 non-null int64\n",
      "sqft_basement    21613 non-null int64\n",
      "yr_built         21613 non-null int64\n",
      "yr_renovated     21613 non-null int64\n",
      "zipcode          21613 non-null int64\n",
      "lat              21613 non-null float64\n",
      "long             21613 non-null float64\n",
      "sqft_living15    21613 non-null int64\n",
      "sqft_lot15       21613 non-null int64\n",
      "dtypes: float64(5), int64(15), object(1)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy array\n",
    "housing_arr = np.array(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting data type with np array\n",
    "\n",
    "#convert price olumns to float32\n",
    "price = np.array(housing[\"price\"],np.float32)\n",
    "\n",
    "#convert waterfrount column to Boolean\n",
    "# waterfront is a np array after conversion\n",
    "waterfront = np.array(housing[\"waterfront\"], np.bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting data type using case in TensorFlow\n",
    "\n",
    "#convert price olumns to float32\n",
    "price = tf.cast(housing[\"price\"],tf.float32)\n",
    "\n",
    "#convert waterfrount column to Boolean\n",
    "# waterfront is tf.tensor type after conversion\n",
    "waterfront = tf.cast(housing[\"waterfront\"],tf.bool)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[221900. 538000. 180000. ... 402101. 400000. 325000.]\n",
      "tf.Tensor([False False False ... False False False], shape=(21613,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# Import numpy and tensorflow with their standard aliases\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Use a numpy array to define price as a 32-bit float\n",
    "price = np.array(housing['price'], np.float32)\n",
    "\n",
    "# Define waterfront as a Boolean using cast\n",
    "waterfront = tf.cast(housing['waterfront'], tf.bool)\n",
    "\n",
    "# Print price and waterfront\n",
    "print(price)\n",
    "print(waterfront)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Loss functions\n",
    "\n",
    "We need the loss funtion to train models because they tell us how well our model explains the data.     \n",
    "Measure of model fit: w/o this feedback, it is unclear how to adjust model parameters during the training process.     \n",
    "High loss value indicates that the model fit is poor: we typically want to minimise the loss function. (in some case we also might want to maximise a function instead). We can always place a minus sign before the function we want to maximize and instead minimise it.      \n",
    "\n",
    "TensorFlow has operations for common loss functions (for linear model):       \n",
    "1) Mean squared error (MSE)           \n",
    "2) Mean absolute error (MAE)        \n",
    "3) Huber error           \n",
    "\n",
    "Loss functions are accessible from tf.keras.losses():       \n",
    "tf.keras.losses.mse()         \n",
    "tf.keras.losses.mae()      \n",
    "tf.keras.losses.Huber()       \n",
    "\n",
    "MSE:       \n",
    "Strongly penalise outliers      \n",
    "High sensitivity near minimum       \n",
    "\n",
    "MAE:    \n",
    "Scales linearly with size of error      \n",
    "Low sensitity near minimum       \n",
    "\n",
    "Huber:    \n",
    "Similar to MSE near minimum      \n",
    "Similar to MAE away form minimum        \n",
    "\n",
    "For greater sensitivity near the minimum, use MSE or Huber.      \n",
    "To minimise the impact of outliers, use MAE or Huber.          \n",
    "\n",
    "In many cases, the training process will require us to supply a function that accepts our model's variables and ata and returns a loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function\n",
    "\n",
    "# compute the MSE loss.\n",
    "# need two tensors to compute it: the actual values or target\n",
    "# predicted values or predictions\n",
    "\n",
    "loss = tf.keras.losses.mse(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a linear regression model\n",
    "def linear_regression(intercept, slope = slope, features = features):\n",
    "    return intercept + features*slope\n",
    "\n",
    "# define a loss function ot compute the MSE\n",
    "def loss_function(intercept, slope, targets = targets, features = features):\n",
    "    # compute the predictions for a linear model\n",
    "    predictions = linear_regression(intercept, slope)\n",
    "    \n",
    "    # return the loss\n",
    "    return tf.keras.losses.mse(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss for test data inputs\n",
    "loss_function(intercept, slope, test_targets, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a variable named scalar\n",
    "scalar = tf.Variable(1.0, dtype=tf.float32)\n",
    "\n",
    "# Define the model\n",
    "def model(scalar, features = features):\n",
    "  \treturn scalar * features\n",
    "\n",
    "# Define a loss function\n",
    "def loss_function(scalar, features = features, targets = targets):\n",
    "\t# Compute the predicted values\n",
    "\tpredictions = model(scalar, features)\n",
    "    \n",
    "\t# Return the mean absolute error loss\n",
    "\treturn tf.keras.losses.mae(targets, predictions)\n",
    "\n",
    "# Evaluate the loss function and print the loss\n",
    "print(loss_function(scalar).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Linear Regression\n",
    "\n",
    "A linear regression model assumes a linear relationship (univariate regression):       \n",
    "price = intercept + size * slope + error (to predict house price with size of house)      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424840500000.0\n",
      "305881000000.0\n",
      "219757840000.0\n",
      "160266650000.0\n",
      "121212770000.0\n",
      "97023190000.0\n",
      "82984410000.0\n",
      "75397860000.0\n",
      "71600530000.0\n",
      "69847540000.0\n",
      "246.97383 253.7275\n"
     ]
    }
   ],
   "source": [
    "# Linear regression in TensorFlow\n",
    "\n",
    "# Define the targets and features\n",
    "price = np.array(housing[\"price\"],np.float32)\n",
    "size = np.array(housing[\"sqft_living\"],np.float32)\n",
    "\n",
    "# Define the intercept and slope, initialisation\n",
    "intercept = tf.Variable(0.1, dtype=np.float32)\n",
    "slope = tf.Variable(0.1, dtype=np.float32)\n",
    "\n",
    "# define a model: linear regression\n",
    "def linear_regression(intercept, slope, features = size):\n",
    "    return intercept + features*slope\n",
    "\n",
    "# compute the predicted and loss\n",
    "def loss_function(intercept, slope, targets = price, features = size):\n",
    "    predictions = linear_regression(intercept, slope)\n",
    "    return tf.keras.losses.mse(targets, predictions)\n",
    "\n",
    "# Define an optimisation operation\n",
    "# learning rate = 0.5\n",
    "opt = tf.keras.optimizers.Adam(0.5)\n",
    "\n",
    "# minimise the loss function and price the loss\n",
    "for j in range(1000):\n",
    "    opt.minimize(lambda: loss_function(intercept,slope), var_list=[intercept,slope])\n",
    "    if j % 100 == 0:\n",
    "\t    print(loss_function(intercept, slope).numpy())\n",
    "    \n",
    "# print the trained parameter\n",
    "print(intercept.numpy(), slope.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Batch training\n",
    "\n",
    "We can use batch training to handle large datasets.        \n",
    "\n",
    "If the dataset is much larger and we cannot fit the entire dataset in memory, we can instead divide it into batchs and train on those batches sequentially.        \n",
    "\n",
    "A single pass over all of the batch is called an epoch and the process itself is called batch training.      \n",
    "Batch trianing also allow us to update model weights and optimizer parameters after each batch, rather than at the end of the epoch.       \n",
    "\n",
    "pd.read_csv() allows us to load data in batches.      \n",
    "Avoid loading entire dataset.     \n",
    "We can do this by using the chunksize parameter to provide batch size.         \n",
    "\n",
    "Full sample:       \n",
    "1) One update per epoch         \n",
    "2) Accepts datasets w/o modification       \n",
    "3) Limited by memory      \n",
    "\n",
    "Batch Training:       \n",
    "1) Multiple updates per epoch       \n",
    "2) Requires division of dataset      \n",
    "3) No limit on dataset size          \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data in batches\n",
    "\n",
    "for batch in pd.read_csv(\"../Machine_Learning_basics/data/kc_house_data.csv\", chunksize=100):\n",
    "    #extract price column\n",
    "    price = np.array(batch[\"price\"],np.float32)\n",
    "    \n",
    "    #extract size column\n",
    "    size = np.array(batch[\"size\"],np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31781912 0.29831016\n"
     ]
    }
   ],
   "source": [
    "# train a linear model in batches, minimumal example\n",
    "\n",
    "# Define trainable variable\n",
    "intercept = tf.Variable(0.1, dtype=np.float32)\n",
    "slope = tf.Variable(0.1,dtype=np.float32)\n",
    "\n",
    "# Define model\n",
    "def linear_regression(intercept, slope, features):\n",
    "    return intercept + features*slope\n",
    "\n",
    "# Compute predicted values and return loss function\n",
    "def loss_function(intercept, slope,targets,features):\n",
    "    predictions = linear_regression(intercept,slope,features)\n",
    "    return tf.keras.losses.mse(targets,predictions)\n",
    "\n",
    "# Define optimizer\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "# load data in batches\n",
    "for batch in pd.read_csv(\"../Machine_Learning_basics/data/kc_house_data.csv\", chunksize=100):\n",
    "    #extract the target and feature columns\n",
    "    price_batch = np.array(batch[\"price\"],np.float32)\n",
    "    size_batch = np.array(batch[\"sqft_lot\"],np.float32)\n",
    "    \n",
    "    #minimize the loss function\n",
    "    opt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch),var_list=[intercept,slope])\n",
    "    \n",
    "print(intercept.numpy(),slope.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Dense layer\n",
    "\n",
    "Input layer consist of features. The output layer contains our prediction. Each hidden layer takes inputs form the previous layer, applies numerical weights to them, sum them together, and then applies an activation function.     \n",
    "\n",
    "Dense layer: applies weights to all nodes from the previous layer.      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs (features)\n",
    "inputs = tf.constant([[1.0,35.0]]) # 2 input (feature) value (e.g. maritial status (1/0) and age)\n",
    "\n",
    "# Define weights\n",
    "weights = tf.Variable([[-0.5],[-0.01]])\n",
    "\n",
    "# Define bias\n",
    "# play a similar role to intercept in the linear regression model\n",
    "bias = tf.Variable([0.5])\n",
    "\n",
    "# A simple dense layer\n",
    "# multiply inputs (features) by the weights\n",
    "product = tf.matmul(inputs, weights)\n",
    "\n",
    "# Define dense layer\n",
    "# add the \"product\" to bias and apply non-linear transformation (sigmoid function, activation function)\n",
    "dense = tf.keras.activations.sigmoid(product+bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A complete model \n",
    "# using high level API Keras\n",
    "\n",
    "# Define input (features) layer\n",
    "inputs = tf.constant(data, tf.float32)\n",
    "\n",
    "# Define first dense layer\n",
    "# 10 outgoing nodes\n",
    "# by default, a bias will be included\n",
    "# inputs as arg to the first dens layer\n",
    "dense1 = tf.keras.layers.Dense(10,activation=\"sigmoid\")(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = tf.keras.layers.Dense(5,activation=\"sigmoid\")(dense1)\n",
    "\n",
    "# Define output (predictions)\n",
    "outputs = tf.keras.layers.Dense(1,activation=\"sigmoid\")(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-level approach:         \n",
    "Relies on complex operations in high-level APIs, such as Keras and Estimators, reducing the amount of code needed. The weights and the mathmatical operations will typically be hidden by the layer constructor.       \n",
    "\n",
    "Low-level approach:         \n",
    "Linear algebra       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer contains 3 features -- education, marital status, and age -- which are available as borrower_features. The hidden layer contains 2 nodes and the output layer contains a single node.          \n",
    "\n",
    "For each layer, you will take the previous layer as an input, initialize a set of weights, compute the product of the inputs and weights, and then apply an activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " dense1's output shape: (1, 2)\n",
      "\n",
      " prediction: 0.9525741338729858\n",
      "\n",
      " actual: 1\n"
     ]
    }
   ],
   "source": [
    "borrower_features = np.array([[2.,  1., 24.]],dtype=np.float32)\n",
    "\n",
    "# Initialize bias1\n",
    "bias1 = tf.Variable(1.0)\n",
    "\n",
    "# Initialize weights1 as 3x2 variable of ones\n",
    "weights1 = tf.Variable(ones((3, 2)))\n",
    "\n",
    "# Perform matrix multiplication of borrower_features and weights1\n",
    "product1 = tf.matmul(borrower_features,weights1)\n",
    "\n",
    "# Apply sigmoid activation function to product1 + bias1\n",
    "dense1 = tf.keras.activations.sigmoid(product1 + bias1)\n",
    "\n",
    "# Print shape of dense1\n",
    "print(\"\\n dense1's output shape: {}\".format(dense1.shape))\n",
    "\n",
    "# Initialize bias2 and weights2\n",
    "bias2 = tf.Variable(1.0)\n",
    "weights2 = tf.Variable(ones((2, 1)))\n",
    "\n",
    "# Perform matrix multiplication of dense1 and weights2\n",
    "product2 = tf.matmul(dense1,weights2)\n",
    "\n",
    "# Apply activation to product2 + bias2 and print the prediction\n",
    "prediction = tf.keras.activations.sigmoid(product2 + bias2)\n",
    "print('\\n prediction: {}'.format(prediction.numpy()[0,0]))\n",
    "print('\\n actual: 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " shape of borrower_features:  (5, 3)\n",
      "\n",
      " shape of weights1:  (3, 2)\n",
      "\n",
      " shape of bias1:  (1,)\n",
      "\n",
      " shape of dense1:  (5, 2)\n"
     ]
    }
   ],
   "source": [
    "weights1 = np.array(\n",
    "       [[-0.6 ,  0.6 ],\n",
    "       [ 0.8 , -0.3 ],\n",
    "       [-0.09, -0.08]],dtype=np.float32)\n",
    "\n",
    "borrower_features = np.array(\n",
    "       [[ 3.,  3., 23.],\n",
    "       [ 2.,  1., 24.],\n",
    "       [ 1.,  1., 49.],\n",
    "       [ 1.,  1., 49.],\n",
    "       [ 2.,  1., 29.]],dtype=np.float32)\n",
    "\n",
    "bias1 = np.array([0.1], dtype=np.float32)\n",
    "# Compute the product of borrower_features and weights1\n",
    "products1 = tf.matmul(borrower_features,weights1)\n",
    "\n",
    "# Apply a sigmoid activation function to products1 + bias1\n",
    "dense1 = tf.keras.activations.sigmoid(products1+bias1)\n",
    "\n",
    "# Print the shapes of borrower_features, weights1, bias1, and dense1\n",
    "print('\\n shape of borrower_features: ', borrower_features.shape)\n",
    "print('\\n shape of weights1: ', weights1.shape)\n",
    "print('\\n shape of bias1: ', bias1.shape)\n",
    "print('\\n shape of dense1: ', dense1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now seen how to define dense layers in tensorflow using linear algebra. In this exercise, we'll skip the linear algebra and let keras work out the details. This will allow us to construct the network below, which has 2 hidden layers and 10 features, using less code than we needed for the network with 1 hidden layer and 3 features.       \n",
    "\n",
    "To construct this network, we'll need to define three dense layers, each of which takes the previous layer as an input, multiplies it by weights, and applies an activation function. Note that input data has been defined and is available as a 100x10 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " shape of dense1:  (100, 7)\n",
      "\n",
      " shape of dense2:  (100, 3)\n",
      "\n",
      " shape of predictions:  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "borrower_features = np.array([[6.96469188e-01, 2.86139339e-01, 2.26851448e-01, 5.51314771e-01,\n",
    "        7.19468951e-01, 4.23106462e-01, 9.80764210e-01, 6.84829712e-01,\n",
    "        4.80931908e-01, 3.92117530e-01],\n",
    "       [3.43178004e-01, 7.29049683e-01, 4.38572258e-01, 5.96778952e-02,\n",
    "        3.98044258e-01, 7.37995386e-01, 1.82491735e-01, 1.75451756e-01,\n",
    "        5.31551361e-01, 5.31827569e-01],\n",
    "       [6.34400964e-01, 8.49431813e-01, 7.24455297e-01, 6.11023486e-01,\n",
    "        7.22443402e-01, 3.22958916e-01, 3.61788660e-01, 2.28263229e-01,\n",
    "        2.93714046e-01, 6.30976140e-01],\n",
    "       [9.21049416e-02, 4.33701187e-01, 4.30862755e-01, 4.93685097e-01,\n",
    "        4.25830305e-01, 3.12261224e-01, 4.26351309e-01, 8.93389165e-01,\n",
    "        9.44160044e-01, 5.01836658e-01],\n",
    "       [6.23952925e-01, 1.15618393e-01, 3.17285478e-01, 4.14826214e-01,\n",
    "        8.66309166e-01, 2.50455379e-01, 4.83034253e-01, 9.85559762e-01,\n",
    "        5.19485116e-01, 6.12894535e-01],\n",
    "       [1.20628662e-01, 8.26340795e-01, 6.03060126e-01, 5.45068026e-01,\n",
    "        3.42763841e-01, 3.04120779e-01, 4.17022198e-01, 6.81300759e-01,\n",
    "        8.75456870e-01, 5.10422349e-01],\n",
    "       [6.69313788e-01, 5.85936546e-01, 6.24903500e-01, 6.74689054e-01,\n",
    "        8.42342436e-01, 8.31949860e-02, 7.63682842e-01, 2.43666381e-01,\n",
    "        1.94222957e-01, 5.72456956e-01],\n",
    "       [9.57125202e-02, 8.85326803e-01, 6.27248943e-01, 7.23416328e-01,\n",
    "        1.61292069e-02, 5.94431877e-01, 5.56785166e-01, 1.58959642e-01,\n",
    "        1.53070509e-01, 6.95529521e-01],\n",
    "       [3.18766415e-01, 6.91970289e-01, 5.54383278e-01, 3.88950586e-01,\n",
    "        9.25132513e-01, 8.41669977e-01, 3.57397556e-01, 4.35914621e-02,\n",
    "        3.04768085e-01, 3.98185670e-01],\n",
    "       [7.04958856e-01, 9.95358467e-01, 3.55914861e-01, 7.62547791e-01,\n",
    "        5.93176901e-01, 6.91701770e-01, 1.51127458e-01, 3.98876280e-01,\n",
    "        2.40855902e-01, 3.43456000e-01],\n",
    "       [5.13128161e-01, 6.66624546e-01, 1.05908483e-01, 1.30894944e-01,\n",
    "        3.21980596e-01, 6.61564350e-01, 8.46506238e-01, 5.53257346e-01,\n",
    "        8.54452491e-01, 3.84837806e-01],\n",
    "       [3.16787899e-01, 3.54264677e-01, 1.71081826e-01, 8.29112649e-01,\n",
    "        3.38670850e-01, 5.52370071e-01, 5.78551471e-01, 5.21533072e-01,\n",
    "        2.68806447e-03, 9.88345444e-01],\n",
    "       [9.05341566e-01, 2.07635865e-01, 2.92489409e-01, 5.20010173e-01,\n",
    "        9.01911378e-01, 9.83630896e-01, 2.57542074e-01, 5.64359069e-01,\n",
    "        8.06968689e-01, 3.94370049e-01],\n",
    "       [7.31073022e-01, 1.61069021e-01, 6.00698590e-01, 8.65864456e-01,\n",
    "        9.83521581e-01, 7.93657899e-02, 4.28347290e-01, 2.04542860e-01,\n",
    "        4.50636476e-01, 5.47763586e-01],\n",
    "       [9.33267102e-02, 2.96860784e-01, 9.27584231e-01, 5.69003761e-01,\n",
    "        4.57412004e-01, 7.53525972e-01, 7.41862178e-01, 4.85790335e-02,\n",
    "        7.08697379e-01, 8.39243352e-01],\n",
    "       [1.65937886e-01, 7.80997932e-01, 2.86536604e-01, 3.06469738e-01,\n",
    "        6.65261447e-01, 1.11392170e-01, 6.64872468e-01, 8.87856781e-01,\n",
    "        6.96311295e-01, 4.40327883e-01],\n",
    "       [4.38214391e-01, 7.65096068e-01, 5.65641999e-01, 8.49041641e-02,\n",
    "        5.82671106e-01, 8.14843714e-01, 3.37066382e-01, 9.27576602e-01,\n",
    "        7.50716984e-01, 5.74063838e-01],\n",
    "       [7.51644015e-01, 7.91489631e-02, 8.59389067e-01, 8.21504116e-01,\n",
    "        9.09871638e-01, 1.28631204e-01, 8.17800835e-02, 1.38415575e-01,\n",
    "        3.99378717e-01, 4.24306870e-01],\n",
    "       [5.62218368e-01, 1.22243546e-01, 2.01399505e-01, 8.11644375e-01,\n",
    "        4.67987567e-01, 8.07938218e-01, 7.42637832e-03, 5.51592708e-01,\n",
    "        9.31932151e-01, 5.82175434e-01],\n",
    "       [2.06095725e-01, 7.17757583e-01, 3.78985852e-01, 6.68383956e-01,\n",
    "        2.93197222e-02, 6.35900378e-01, 3.21979336e-02, 7.44780660e-01,\n",
    "        4.72912997e-01, 1.21754356e-01],\n",
    "       [5.42635918e-01, 6.67744428e-02, 6.53364897e-01, 9.96086299e-01,\n",
    "        7.69397318e-01, 5.73774099e-01, 1.02635257e-01, 6.99834049e-01,\n",
    "        6.61167860e-01, 4.90971319e-02],\n",
    "       [7.92299330e-01, 5.18716574e-01, 4.25867707e-01, 7.88187146e-01,\n",
    "        4.11569238e-01, 4.81026262e-01, 1.81628838e-01, 3.21318895e-01,\n",
    "        8.45533013e-01, 1.86903745e-01],\n",
    "       [4.17291075e-01, 9.89034534e-01, 2.36599818e-01, 9.16832328e-01,\n",
    "        9.18397486e-01, 9.12963450e-02, 4.63652730e-01, 5.02216339e-01,\n",
    "        3.13668936e-01, 4.73395362e-02],\n",
    "       [2.41685644e-01, 9.55296382e-02, 2.38249913e-01, 8.07791114e-01,\n",
    "        8.94978285e-01, 4.32228930e-02, 3.01946849e-01, 9.80582178e-01,\n",
    "        5.39504826e-01, 6.26309335e-01],\n",
    "       [5.54540846e-03, 4.84909445e-01, 9.88328516e-01, 3.75185519e-01,\n",
    "        9.70381573e-02, 4.61908758e-01, 9.63004470e-01, 3.41830611e-01,\n",
    "        7.98922718e-01, 7.98846304e-01],\n",
    "       [2.08248302e-01, 4.43367690e-01, 7.15601265e-01, 4.10519779e-01,\n",
    "        1.91006958e-01, 9.67494309e-01, 6.50750339e-01, 8.65459859e-01,\n",
    "        2.52423584e-02, 2.66905814e-01],\n",
    "       [5.02071083e-01, 6.74486384e-02, 9.93033290e-01, 2.36462399e-01,\n",
    "        3.74292195e-01, 2.14011908e-01, 1.05445869e-01, 2.32479781e-01,\n",
    "        3.00610125e-01, 6.34442270e-01],\n",
    "       [2.81234771e-01, 3.62276763e-01, 5.94284385e-03, 3.65719140e-01,\n",
    "        5.33885956e-01, 1.62015840e-01, 5.97433090e-01, 2.93152481e-01,\n",
    "        6.32050514e-01, 2.61966046e-02],\n",
    "       [8.87593448e-01, 1.61186308e-02, 1.26958027e-01, 7.77162433e-01,\n",
    "        4.58952338e-02, 7.10998714e-01, 9.71046150e-01, 8.71682942e-01,\n",
    "        7.10161626e-01, 9.58509743e-01],\n",
    "       [4.29813325e-01, 8.72878909e-01, 3.55957657e-01, 9.29763675e-01,\n",
    "        1.48777649e-01, 9.40029025e-01, 8.32716227e-01, 8.46054852e-01,\n",
    "        1.23923011e-01, 5.96486926e-01],\n",
    "       [1.63924806e-02, 7.21184373e-01, 7.73751410e-03, 8.48222747e-02,\n",
    "        2.25498408e-01, 8.75124514e-01, 3.63576323e-01, 5.39959908e-01,\n",
    "        5.68103194e-01, 2.25463361e-01],\n",
    "       [5.72146773e-01, 6.60951793e-01, 2.98245400e-01, 4.18626845e-01,\n",
    "        4.53088939e-01, 9.32350636e-01, 5.87493777e-01, 9.48252380e-01,\n",
    "        5.56034744e-01, 5.00561416e-01],\n",
    "       [3.53221106e-03, 4.80889052e-01, 9.27455008e-01, 1.98365688e-01,\n",
    "        5.20911328e-02, 4.06778902e-01, 3.72396469e-01, 8.57153058e-01,\n",
    "        2.66111158e-02, 9.20149207e-01],\n",
    "       [6.80903018e-01, 9.04226005e-01, 6.07529044e-01, 8.11953306e-01,\n",
    "        3.35543871e-01, 3.49566221e-01, 3.89874220e-01, 7.54797101e-01,\n",
    "        3.69291186e-01, 2.42219806e-01],\n",
    "       [9.37668383e-01, 9.08011079e-01, 3.48797321e-01, 6.34638071e-01,\n",
    "        2.73842216e-01, 2.06115127e-01, 3.36339533e-01, 3.27099890e-01,\n",
    "        8.82276118e-01, 8.22303832e-01],\n",
    "       [7.09623218e-01, 9.59345222e-01, 4.22543347e-01, 2.45033041e-01,\n",
    "        1.17398441e-01, 3.01053345e-01, 1.45263731e-01, 9.21861008e-02,\n",
    "        6.02932215e-01, 3.64187449e-01],\n",
    "       [5.64570367e-01, 1.91335723e-01, 6.76905870e-01, 2.15505451e-01,\n",
    "        2.78023601e-01, 7.41760433e-01, 5.59737921e-01, 3.34836423e-01,\n",
    "        5.42988777e-01, 6.93984687e-01],\n",
    "       [9.12132144e-01, 5.80713212e-01, 2.32686386e-01, 7.46697605e-01,\n",
    "        7.77769029e-01, 2.00401321e-01, 8.20574224e-01, 4.64934856e-01,\n",
    "        7.79766679e-01, 2.37478226e-01],\n",
    "       [3.32580268e-01, 9.53697145e-01, 6.57815099e-01, 7.72877812e-01,\n",
    "        6.88374341e-01, 2.04304114e-01, 4.70688760e-01, 8.08963895e-01,\n",
    "        6.75035119e-01, 6.02788571e-03],\n",
    "       [8.74077454e-02, 3.46794724e-01, 9.44365561e-01, 4.91190493e-01,\n",
    "        2.70176262e-01, 3.60423714e-01, 2.10652635e-01, 4.21200067e-01,\n",
    "        2.18035445e-01, 8.45752478e-01],\n",
    "       [4.56270605e-01, 2.79802024e-01, 9.32891667e-01, 3.14351350e-01,\n",
    "        9.09714639e-01, 4.34180908e-02, 7.07115054e-01, 4.83889043e-01,\n",
    "        4.44221050e-01, 3.63233462e-02],\n",
    "       [4.06831913e-02, 3.32753628e-01, 9.47119534e-01, 6.17659986e-01,\n",
    "        3.68874848e-01, 6.11977041e-01, 2.06131533e-01, 1.65066436e-01,\n",
    "        3.61817271e-01, 8.63353372e-01],\n",
    "       [5.09401739e-01, 2.96901524e-01, 9.50251639e-01, 8.15966070e-01,\n",
    "        3.22973937e-01, 9.72098231e-01, 9.87351120e-01, 4.08660144e-01,\n",
    "        6.55923128e-01, 4.05653208e-01],\n",
    "       [2.57348120e-01, 8.26526731e-02, 2.63610333e-01, 2.71479845e-01,\n",
    "        3.98639083e-01, 1.84886038e-01, 9.53818381e-01, 1.02879882e-01,\n",
    "        6.25208557e-01, 4.41697389e-01],\n",
    "       [4.23518062e-01, 3.71991783e-01, 8.68314683e-01, 2.80476987e-01,\n",
    "        2.05761567e-02, 9.18097019e-01, 8.64480257e-01, 2.76901782e-01,\n",
    "        5.23487568e-01, 1.09088197e-01],\n",
    "       [9.34270695e-02, 8.37466121e-01, 4.10265714e-01, 6.61716521e-01,\n",
    "        9.43200588e-01, 2.45130599e-01, 1.31598311e-02, 2.41484065e-02,\n",
    "        7.09385693e-01, 9.24551904e-01],\n",
    "       [4.67330277e-01, 3.75109136e-01, 5.42860448e-01, 8.58916819e-01,\n",
    "        6.52153850e-01, 2.32979894e-01, 7.74580181e-01, 1.34613499e-01,\n",
    "        1.65559977e-01, 6.12682283e-01],\n",
    "       [2.38783404e-01, 7.04778552e-01, 3.49518538e-01, 2.77423948e-01,\n",
    "        9.98918414e-01, 4.06161249e-02, 6.45822525e-01, 3.86995859e-02,\n",
    "        7.60210276e-01, 2.30089962e-01],\n",
    "       [8.98318663e-02, 6.48449719e-01, 7.32601225e-01, 6.78095341e-01,\n",
    "        5.19009456e-02, 2.94306934e-01, 4.51088339e-01, 2.87103295e-01,\n",
    "        8.10513437e-01, 1.31115109e-01],\n",
    "       [6.12179339e-01, 9.88214970e-01, 9.02556539e-01, 2.22157061e-01,\n",
    "        8.18876142e-05, 9.80597317e-01, 8.82712960e-01, 9.19472456e-01,\n",
    "        4.15503561e-01, 7.44615436e-01],\n",
    "       [2.12831497e-01, 3.92304063e-01, 8.51548076e-01, 1.27612218e-01,\n",
    "        8.93865347e-01, 4.96507972e-01, 4.26095665e-01, 3.05646390e-01,\n",
    "        9.16848779e-01, 5.17623484e-01],\n",
    "       [8.04026365e-01, 8.57651770e-01, 9.22382355e-01, 3.03380728e-01,\n",
    "        3.39810848e-01, 5.95073879e-01, 4.41324145e-01, 9.32842553e-01,\n",
    "        3.97564054e-01, 4.77778047e-01],\n",
    "       [6.17186069e-01, 4.04739499e-01, 9.92478430e-01, 9.88512859e-02,\n",
    "        2.20603317e-01, 3.22655141e-01, 1.47722840e-01, 2.84219235e-01,\n",
    "        7.79245317e-01, 5.22891998e-01],\n",
    "       [3.39536369e-02, 9.82622564e-01, 6.16006494e-01, 5.89394793e-02,\n",
    "        6.61168754e-01, 3.78369361e-01, 1.35673299e-01, 5.63664615e-01,\n",
    "        7.27079928e-01, 6.71126604e-01],\n",
    "       [2.47513160e-01, 5.24866223e-01, 5.37663460e-01, 7.16803372e-01,\n",
    "        3.59867334e-01, 7.97732592e-01, 6.27921820e-01, 3.83316055e-02,\n",
    "        5.46479046e-01, 8.61912072e-01],\n",
    "       [5.67574143e-01, 1.75828263e-01, 5.10376394e-01, 7.56945848e-01,\n",
    "        1.10105194e-01, 8.17099094e-01, 1.67481646e-01, 5.34076512e-01,\n",
    "        3.85743469e-01, 2.48623773e-01],\n",
    "       [6.47432506e-01, 3.73921096e-02, 7.60045826e-01, 5.26940644e-01,\n",
    "        8.75771224e-01, 5.20718336e-01, 3.50331701e-02, 1.43600971e-01,\n",
    "        7.95604587e-01, 4.91976053e-01],\n",
    "       [4.41879272e-01, 3.18434775e-01, 2.84549206e-01, 9.65886295e-01,\n",
    "        4.32969332e-01, 8.84003043e-01, 6.48163140e-01, 8.58427644e-01,\n",
    "        8.52449536e-01, 9.56312001e-01],\n",
    "       [6.97942257e-01, 8.05396914e-01, 7.33127892e-01, 6.05226815e-01,\n",
    "        7.17354119e-01, 7.15750396e-01, 4.09077927e-02, 5.16110837e-01,\n",
    "        7.92651355e-01, 2.42962182e-01],\n",
    "       [4.65147972e-01, 4.34985697e-01, 4.02787179e-01, 1.21839531e-01,\n",
    "        5.25711536e-01, 4.46248353e-01, 6.63392782e-01, 5.49413085e-01,\n",
    "        2.75429301e-02, 3.19179893e-02],\n",
    "       [7.01359808e-01, 7.07581103e-01, 9.59939122e-01, 8.76704693e-01,\n",
    "        4.68059659e-01, 6.25906527e-01, 4.57181722e-01, 2.22946241e-01,\n",
    "        3.76677006e-01, 1.03884235e-01],\n",
    "       [6.66527092e-01, 1.92030147e-01, 4.75467801e-01, 9.67436612e-01,\n",
    "        3.16689312e-02, 1.51729956e-01, 2.98579186e-01, 9.41806972e-01,\n",
    "        9.08841789e-01, 1.62000835e-01],\n",
    "       [9.81117785e-01, 7.50747502e-01, 5.39977074e-01, 9.31702912e-01,\n",
    "        8.80607128e-01, 3.91316503e-01, 6.56343222e-01, 6.47385120e-01,\n",
    "        3.26968193e-01, 1.79390177e-01],\n",
    "       [4.66809869e-01, 2.63281047e-01, 3.55065137e-01, 9.54143941e-01,\n",
    "        4.61137861e-01, 6.84891462e-01, 3.36229891e-01, 9.95861053e-01,\n",
    "        6.58767581e-01, 1.96009472e-01],\n",
    "       [9.81839970e-02, 9.43180561e-01, 9.44777846e-01, 6.21328354e-01,\n",
    "        1.69914998e-02, 2.25534886e-01, 8.01276803e-01, 8.75459850e-01,\n",
    "        4.53989804e-01, 3.65520626e-01],\n",
    "       [2.74224997e-01, 1.16970517e-01, 1.15744539e-01, 9.52602684e-01,\n",
    "        8.08626115e-01, 1.64779365e-01, 2.07050055e-01, 6.55551553e-01,\n",
    "        7.64664233e-01, 8.10314834e-01],\n",
    "       [1.63337693e-01, 9.84128296e-01, 2.27802068e-01, 5.89415431e-01,\n",
    "        5.87615728e-01, 9.67361867e-01, 6.57667458e-01, 5.84904253e-01,\n",
    "        5.18772602e-01, 7.64657557e-01],\n",
    "       [1.06055260e-01, 2.09190114e-03, 9.52488840e-01, 4.98657674e-01,\n",
    "        3.28335375e-01, 3.68053257e-01, 8.03843319e-01, 3.82370204e-01,\n",
    "        7.70169199e-01, 4.40461993e-01],\n",
    "       [8.44077468e-01, 7.62040615e-02, 4.81128335e-01, 4.66849715e-01,\n",
    "        2.64327973e-01, 9.43614721e-01, 9.05028462e-01, 4.43596303e-01,\n",
    "        9.71596092e-02, 2.06783146e-01],\n",
    "       [2.71491826e-01, 4.84219760e-01, 3.38377118e-01, 7.74136066e-01,\n",
    "        4.76026595e-01, 8.70370507e-01, 9.95781779e-01, 2.19835952e-01,\n",
    "        6.11671388e-01, 8.47502291e-01],\n",
    "       [9.45236623e-01, 2.90086418e-01, 7.27042735e-01, 1.50161488e-02,\n",
    "        8.79142463e-01, 6.39385507e-02, 7.33395398e-01, 9.94610369e-01,\n",
    "        5.01189768e-01, 2.09333986e-01],\n",
    "       [5.94643593e-01, 6.24149978e-01, 6.68072760e-01, 1.72611743e-01,\n",
    "        8.98712695e-01, 6.20991349e-01, 4.35687043e-02, 6.84041083e-01,\n",
    "        1.96084052e-01, 2.73407809e-02],\n",
    "       [5.50953269e-01, 8.13313663e-01, 8.59941125e-01, 1.03520922e-01,\n",
    "        6.63042784e-01, 7.10075200e-01, 2.94516981e-01, 9.71364021e-01,\n",
    "        2.78687477e-01, 6.99821860e-02],\n",
    "       [5.19280374e-01, 6.94314897e-01, 2.44659781e-01, 3.38582188e-01,\n",
    "        5.63627958e-01, 8.86678159e-01, 7.47325897e-01, 2.09591955e-01,\n",
    "        2.51777083e-01, 5.23880661e-01],\n",
    "       [7.68958688e-01, 6.18761778e-01, 5.01324296e-01, 5.97125351e-01,\n",
    "        7.56060004e-01, 5.37079811e-01, 8.97752762e-01, 9.47067499e-01,\n",
    "        9.15354490e-01, 7.54518330e-01],\n",
    "       [2.46321008e-01, 3.85271460e-01, 2.79999942e-01, 6.57660246e-01,\n",
    "        3.24221611e-01, 7.54391611e-01, 1.13509081e-01, 7.75364757e-01,\n",
    "        5.85901976e-01, 8.35388660e-01],\n",
    "       [4.30875659e-01, 6.24964476e-01, 5.54412127e-01, 9.75671291e-01,\n",
    "        7.55474389e-01, 5.44813275e-01, 1.74032092e-01, 9.04114246e-01,\n",
    "        2.05837786e-01, 6.50043249e-01],\n",
    "       [9.36471879e-01, 2.23579630e-01, 2.25923538e-01, 8.51818919e-01,\n",
    "        8.27655017e-01, 3.51703346e-01, 2.65096277e-01, 1.27388477e-01,\n",
    "        9.87936080e-01, 8.35343122e-01],\n",
    "       [8.99391592e-01, 5.13679326e-01, 1.14384830e-01, 5.25803380e-02,\n",
    "        3.30582112e-01, 9.20330405e-01, 9.47581828e-01, 8.41163874e-01,\n",
    "        1.58679143e-01, 4.19923156e-01],\n",
    "       [2.46242926e-01, 2.05349773e-01, 6.84825838e-01, 4.86111671e-01,\n",
    "        3.24909657e-01, 1.00214459e-01, 5.44763386e-01, 3.47025156e-01,\n",
    "        3.91095817e-01, 3.10508728e-01],\n",
    "       [3.87195200e-01, 5.55859566e-01, 1.41438060e-02, 8.47647011e-01,\n",
    "        9.21919882e-01, 5.50529718e-01, 2.68021107e-01, 9.90239024e-01,\n",
    "        3.83194029e-01, 6.93655372e-01],\n",
    "       [6.89952552e-01, 4.34309065e-01, 1.99158162e-01, 9.66579378e-01,\n",
    "        6.36908561e-02, 4.85149384e-01, 2.20730707e-01, 2.93974131e-01,\n",
    "        8.28527331e-01, 3.67265552e-01],\n",
    "       [8.33482668e-02, 1.96309000e-01, 8.60373437e-01, 9.77028847e-01,\n",
    "        2.67982155e-01, 6.75408959e-01, 8.11989978e-02, 7.23465621e-01,\n",
    "        4.16436613e-01, 9.18159902e-01],\n",
    "       [3.11536163e-01, 9.41466987e-01, 5.03247440e-01, 3.48892927e-01,\n",
    "        6.47019625e-01, 2.49746203e-01, 2.29763597e-01, 1.96346447e-01,\n",
    "        9.59899545e-01, 4.92913723e-01],\n",
    "       [7.51614988e-01, 4.73991871e-01, 5.87540150e-01, 5.84138989e-01,\n",
    "        9.79886293e-01, 6.68433130e-01, 2.39769474e-01, 1.51976589e-02,\n",
    "        2.18682140e-01, 4.55519646e-01],\n",
    "       [3.93420339e-01, 8.12326252e-01, 7.85556734e-01, 8.90959650e-02,\n",
    "        9.52010751e-01, 5.27456701e-01, 5.96403956e-01, 4.05056775e-01,\n",
    "        6.49500966e-01, 8.71326327e-01],\n",
    "       [6.73935950e-01, 9.70098555e-01, 7.01122224e-01, 8.21720719e-01,\n",
    "        4.50395830e-02, 6.72698498e-01, 6.54752672e-01, 1.01746053e-01,\n",
    "        8.42387497e-01, 6.14172399e-01],\n",
    "       [9.83280912e-02, 5.94467103e-01, 4.78415847e-01, 2.33293563e-01,\n",
    "        1.97560899e-02, 3.65567267e-01, 6.19851053e-01, 3.29279125e-01,\n",
    "        3.07254642e-01, 7.51121223e-01],\n",
    "       [7.58624673e-01, 7.18765855e-01, 1.01181954e-01, 5.16165972e-01,\n",
    "        5.57798684e-01, 7.44804502e-01, 9.03177738e-01, 3.69038880e-01,\n",
    "        4.28663462e-01, 7.32767463e-01],\n",
    "       [6.62636399e-01, 5.57869911e-01, 3.50139618e-01, 1.95352346e-01,\n",
    "        1.83807373e-01, 8.15832913e-02, 8.12008530e-02, 8.45798194e-01,\n",
    "        3.83672744e-01, 6.07396215e-02],\n",
    "       [8.96425664e-01, 2.23270476e-01, 2.68124431e-01, 1.94497839e-01,\n",
    "        9.67501044e-01, 1.12540089e-01, 7.22163260e-01, 9.32088733e-01,\n",
    "        6.68001294e-01, 8.58726621e-01],\n",
    "       [2.42447108e-01, 6.73927963e-01, 7.00871348e-01, 4.58332509e-01,\n",
    "        8.70545626e-01, 6.94386125e-01, 8.94877791e-01, 7.53204346e-01,\n",
    "        5.20290434e-01, 4.98688221e-01],\n",
    "       [4.53727633e-01, 2.16468628e-02, 5.35141408e-01, 4.22973245e-01,\n",
    "        1.57533601e-01, 1.19069695e-01, 4.49351877e-01, 3.99130546e-02,\n",
    "        9.86579895e-01, 3.78120929e-01],\n",
    "       [3.82109195e-01, 5.11263013e-02, 4.26672339e-01, 1.57454368e-02,\n",
    "        3.00936326e-02, 3.39099228e-01, 8.20968926e-01, 4.58821088e-01,\n",
    "        1.48405796e-02, 1.63220033e-01],\n",
    "       [7.39922702e-01, 7.38293707e-01, 7.54522920e-01, 3.51669371e-01,\n",
    "        3.52276951e-01, 8.02075684e-01, 3.98137897e-01, 7.27191031e-01,\n",
    "        5.81122994e-01, 3.64341676e-01],\n",
    "       [8.00065175e-02, 1.16125375e-01, 8.89558733e-01, 4.52340513e-01,\n",
    "        9.94004548e-01, 3.63896936e-01, 2.49954298e-01, 3.50539327e-01,\n",
    "        3.43086094e-01, 6.37356758e-01],\n",
    "       [1.27375638e-02, 7.63268650e-01, 4.16414618e-01, 4.32239205e-01,\n",
    "        4.81115013e-01, 4.49212462e-01, 4.97470886e-01, 3.45904320e-01,\n",
    "        4.53346133e-01, 4.04651344e-01],\n",
    "       [5.18242717e-01, 6.23269081e-01, 2.41040602e-01, 5.08437157e-01,\n",
    "        5.94621897e-01, 1.69483144e-02, 5.20493746e-01, 2.39293247e-01,\n",
    "        4.04538542e-01, 8.26530159e-01],\n",
    "       [3.26235592e-01, 4.83216912e-01, 2.47411542e-02, 3.08750868e-01,\n",
    "        6.39721096e-01, 3.15161765e-01, 2.05797508e-01, 2.90655673e-01,\n",
    "        9.54378307e-01, 8.68018195e-02],\n",
    "       [4.63357776e-01, 5.83869033e-02, 5.38658261e-01, 1.46035731e-01,\n",
    "        6.34084821e-01, 2.64397472e-01, 6.90915406e-01, 3.47146064e-01,\n",
    "        4.16848855e-03, 2.94894695e-01]],dtype=np.float32)\n",
    "\n",
    "# Define the first dense layer\n",
    "dense1 = tf.keras.layers.Dense(7, activation='sigmoid')(borrower_features)\n",
    "\n",
    "# Define a dense layer with 3 output nodes\n",
    "dense2 = tf.keras.layers.Dense(3,activation=\"sigmoid\")(dense1)\n",
    "\n",
    "# Define a dense layer with 1 output node\n",
    "predictions = tf.keras.layers.Dense(1,activation=\"sigmoid\")(dense2)\n",
    "\n",
    "# Print the shapes of dense1, dense2, and predictions\n",
    "print('\\n shape of dense1: ', dense1.shape)\n",
    "print('\\n shape of dense2: ', dense2.shape)\n",
    "print('\\n shape of predictions: ', predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Activation Functions\n",
    "\n",
    "A typical hidden layer consists of 2 operations:       \n",
    "1) Linear: performs matrix multiplication, which is a linear operation.         \n",
    "2) Nonlinear: activation function.         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# a simple example where we assume that the weight on age is 1 and the weight on the bill amount is 2\n",
    "\n",
    "# Define example borrower features\n",
    "young, old = 0.3, 0.6\n",
    "low_bill, high_bill = 0.1, 0.5\n",
    "\n",
    "# apply matrix multiplication step for all feature combination\n",
    "young_high = 1.0*young + 2.0*high_bill\n",
    "young_low = 1.0*young + 2.0*low_bill\n",
    "old_high = 1.0*old + 2.0*high_bill\n",
    "old_low = 1.0*old + 2.0*low_bill\n",
    "\n",
    "# Difference in default predictions for young\n",
    "print(young_high - young_low)\n",
    "\n",
    "# Difference in default predictions for old\n",
    "print(old_high - old_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16337568\n",
      "0.14204389\n"
     ]
    }
   ],
   "source": [
    "# with activation function\n",
    "print(tf.keras.activations.sigmoid(young_high).numpy()-tf.keras.activations.sigmoid(young_low).numpy())\n",
    "\n",
    "print(tf.keras.activations.sigmoid(old_high).numpy()-tf.keras.activations.sigmoid(old_low).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common activation function: sigmoid, ReLu and softmax.        \n",
    "\n",
    "The sigmoid activation function is used primarily in the output layer of binary classificaiton problem.     \n",
    "Low-level: tf.keras.activations.sigmoid()         \n",
    "High-level: sigmoid       \n",
    "\n",
    "The ReLu activation function (Rectified Linear Unit) is used in all layers other htan the output layer. This activation simply takes the maximum of the value passed to it and 0 if x is negative.          \n",
    "Low-level: tf.keras.activations.relu()         \n",
    "High-level: relu         \n",
    "\n",
    "The softmax activation function is used in the output layer in classification problems with more than two classes. The outputs from a softmax activation function can be interpreted as predicted class probabilities in multiclass classification problems.      \n",
    "Low-level: tf.keras.activations.softmax()       \n",
    "High-level: softmax          \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input layer\n",
    "inputs = tf.constant(borrower_features, dtype = tf.float32)\n",
    "\n",
    "# Define dense layer 1\n",
    "dense1 = tf.keras.layers.Dense(16, activation=\"relu\")(inputs)\n",
    "\n",
    "# Define dense 2\n",
    "dense2 = tf.keras.layers.Dense(8, activation=\"sigmoid\")(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = tf.keras.layers.Dense(4, activation=\"softmax\")(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.1105312  0.19927394 0.20831117 0.48188367]\n",
      " [0.11558039 0.19875197 0.20812516 0.47754243]\n",
      " [0.11165091 0.19399762 0.20439456 0.48995692]\n",
      " [0.11063999 0.19841023 0.20914055 0.48180923]\n",
      " [0.11378781 0.20506497 0.20466019 0.476487  ]\n",
      " [0.11229768 0.19330044 0.20363392 0.490768  ]\n",
      " [0.11060317 0.19343947 0.19988917 0.4960683 ]\n",
      " [0.10830142 0.1926553  0.20854698 0.49049637]\n",
      " [0.11424928 0.19637963 0.20755497 0.48181608]\n",
      " [0.1126796  0.2006236  0.21552572 0.47117105]\n",
      " [0.11160199 0.19368425 0.20495002 0.48976383]\n",
      " [0.1067858  0.20228057 0.21580915 0.47512448]\n",
      " [0.11331404 0.20366342 0.2087831  0.47423944]\n",
      " [0.10725141 0.20448974 0.21608381 0.472175  ]\n",
      " [0.10671645 0.18948689 0.20531829 0.4984784 ]\n",
      " [0.11295969 0.19189045 0.20054823 0.49460164]\n",
      " [0.1190635  0.18990554 0.19749677 0.4935342 ]\n",
      " [0.10769681 0.20565833 0.21392554 0.4727194 ]\n",
      " [0.11045446 0.20933622 0.21675651 0.46345285]\n",
      " [0.11290117 0.20186439 0.21643701 0.46879745]\n",
      " [0.10897365 0.20334868 0.21717392 0.47050372]\n",
      " [0.10657024 0.20340462 0.22399177 0.46603337]\n",
      " [0.11524394 0.20025355 0.2065982  0.47790435]\n",
      " [0.10927545 0.21213399 0.21688128 0.46170932]\n",
      " [0.11036664 0.18265198 0.19542211 0.51155925]\n",
      " [0.11379617 0.18718433 0.1969357  0.5020838 ]\n",
      " [0.11102641 0.19889647 0.20473786 0.48533928]\n",
      " [0.10907462 0.20482072 0.21764213 0.4684625 ]\n",
      " [0.10831576 0.20038567 0.22721376 0.46408483]\n",
      " [0.11026818 0.192733   0.20525138 0.4917474 ]\n",
      " [0.11449803 0.20094211 0.21493039 0.4696296 ]\n",
      " [0.11410032 0.19326259 0.20474693 0.48789018]\n",
      " [0.10985476 0.18866464 0.19837326 0.5031073 ]\n",
      " [0.11321136 0.19442973 0.20186357 0.49049532]\n",
      " [0.10953765 0.20119268 0.2164613  0.47280833]\n",
      " [0.11163966 0.19831316 0.2109841  0.479063  ]\n",
      " [0.10939402 0.19457044 0.20663916 0.48939648]\n",
      " [0.11102261 0.20010066 0.21177487 0.4771018 ]\n",
      " [0.11586583 0.1896351  0.19719    0.49730906]\n",
      " [0.11004435 0.19538265 0.20613818 0.48843482]\n",
      " [0.11259286 0.18762057 0.19084111 0.5089454 ]\n",
      " [0.11080331 0.19615345 0.20705757 0.4859857 ]\n",
      " [0.10542542 0.18458682 0.20642832 0.50355947]\n",
      " [0.10871534 0.20448345 0.21502471 0.47177652]\n",
      " [0.10828221 0.18311366 0.1997028  0.5089013 ]\n",
      " [0.11406558 0.21351369 0.21469232 0.4577284 ]\n",
      " [0.105976   0.19563681 0.21249871 0.48588848]\n",
      " [0.11383094 0.1979527  0.20221707 0.4859993 ]\n",
      " [0.10928926 0.19298689 0.2078866  0.48983726]\n",
      " [0.11492088 0.17568146 0.18353607 0.52586156]\n",
      " [0.1161517  0.19173415 0.1967452  0.49536893]\n",
      " [0.11762445 0.18322083 0.18776965 0.51138514]\n",
      " [0.11301608 0.19140367 0.20136528 0.494215  ]\n",
      " [0.11650822 0.18881771 0.20256431 0.4921098 ]\n",
      " [0.10622111 0.1943939  0.21274565 0.48663938]\n",
      " [0.10721812 0.19858183 0.21792074 0.4762793 ]\n",
      " [0.11129986 0.20624419 0.2088363  0.4736197 ]\n",
      " [0.10846916 0.19926871 0.21417977 0.4780823 ]\n",
      " [0.11543045 0.19499671 0.20785823 0.48171467]\n",
      " [0.11472616 0.19566199 0.19812228 0.49148956]\n",
      " [0.10790893 0.1885262  0.20515496 0.49840993]\n",
      " [0.10811504 0.20040007 0.22345433 0.4680306 ]\n",
      " [0.11208559 0.19506228 0.2009676  0.49188456]\n",
      " [0.10864721 0.201836   0.2184765  0.4710403 ]\n",
      " [0.11228041 0.18114877 0.18674518 0.51982564]\n",
      " [0.10840522 0.21742798 0.21969797 0.45446882]\n",
      " [0.113337   0.19448227 0.2092291  0.48295164]\n",
      " [0.10850785 0.19025658 0.20659482 0.4946407 ]\n",
      " [0.10640541 0.19106723 0.21037553 0.49215183]\n",
      " [0.10540175 0.19286698 0.21223152 0.48949975]\n",
      " [0.11908932 0.18990175 0.1854024  0.5056066 ]\n",
      " [0.11971039 0.19240747 0.20098929 0.48689282]\n",
      " [0.12208233 0.18203628 0.18810953 0.5077719 ]\n",
      " [0.10977089 0.19375172 0.20939168 0.48708567]\n",
      " [0.11202823 0.18902011 0.1989301  0.5000216 ]\n",
      " [0.11045123 0.20636275 0.2167147  0.4664713 ]\n",
      " [0.11373455 0.20032102 0.21054652 0.47539794]\n",
      " [0.10801653 0.21278831 0.21933575 0.4598594 ]\n",
      " [0.11624637 0.19668715 0.19559799 0.49146852]\n",
      " [0.1088906  0.19705929 0.20959784 0.48445234]\n",
      " [0.11406914 0.21041502 0.21529675 0.46021912]\n",
      " [0.10435356 0.20500602 0.23159039 0.45905006]\n",
      " [0.1099856  0.19881228 0.2106762  0.4805259 ]\n",
      " [0.11512661 0.19714265 0.2071206  0.48061016]\n",
      " [0.11138599 0.19953285 0.20854157 0.48053956]\n",
      " [0.11819808 0.18642756 0.19086355 0.5045109 ]\n",
      " [0.10608768 0.18929143 0.21234746 0.49227348]\n",
      " [0.10699471 0.19441234 0.20729628 0.49129668]\n",
      " [0.10927485 0.1982943  0.20905967 0.48337123]\n",
      " [0.11600116 0.2023512  0.20420906 0.47743863]\n",
      " [0.11689461 0.20394148 0.19781536 0.48134857]\n",
      " [0.11547948 0.18472226 0.19126777 0.5085305 ]\n",
      " [0.10677099 0.20266506 0.22550987 0.46505412]\n",
      " [0.11113179 0.19713047 0.20829353 0.4834442 ]\n",
      " [0.1149728  0.18742852 0.19766584 0.49993283]\n",
      " [0.1142473  0.20069808 0.20239753 0.48265702]\n",
      " [0.11345199 0.19635254 0.2065166  0.48367894]\n",
      " [0.11051384 0.20787218 0.21143715 0.47017688]\n",
      " [0.11148044 0.2087094  0.21996862 0.45984155]\n",
      " [0.10961106 0.19684777 0.20342347 0.49011767]], shape=(100, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will again make use of credit card data. The target variable, default, indicates whether a credit card holder defaults on her payment in the following period. Since there are only two options--default or not--this is a binary classification problem. While the dataset has many features, you will focus on just three: the size of the three latest credit card bills. Finally, you will compute predictions from your untrained network, outputs, and compare those the target variable, default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
       "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
       "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
       "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
       "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
       "1  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
       "2  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
       "3  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
       "4  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0       0.0       0.0       0.0                           1  \n",
       "1    1000.0       0.0    2000.0                           1  \n",
       "2    1000.0    1000.0    5000.0                           0  \n",
       "3    1100.0    1069.0    1000.0                           0  \n",
       "4    9000.0     689.0     679.0                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_df = pd.read_csv(\"../Machine_Learning_basics/data/uci_credit_card.csv\")\n",
    "cc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_amounts = np.array(cc_df[[\"BILL_AMT1\",\"BILL_AMT2\",\"BILL_AMT3\"]],np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = np.array(cc_df[[\"default.payment.next.month\"]],np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [ 0.18238378]\n",
      " [-1.        ]\n",
      " [-1.        ]\n",
      " [-1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Construct input layer from features\n",
    "inputs = tf.constant(bill_amounts)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = tf.keras.layers.Dense(3, activation='relu')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = tf.keras.layers.Dense(2,activation=\"relu\")(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = tf.keras.layers.Dense(1,activation=\"sigmoid\")(dense2)\n",
    "\n",
    "# Print error for first five examples\n",
    "error = default[:5] - outputs.numpy()[:5]\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass classification problems       \n",
    "In this exercise, we expand beyond binary classification to cover multiclass problems. A multiclass problem has targets that can take on three or more values. In the credit card dataset, the education variable can take on 6 different values, each corresponding to a different level of education. We will use that as our target in this exercise and will also expand the feature set from 3 to 10 columns.              \n",
    "\n",
    "As in the previous problem, you will define an input layer, dense layers, and an output layer. You will also print the untrained model's predictions, which are probabilities assigned to the classes. The tensor of features has been loaded and is available as borrower_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "borrower_features = np.array(cc_df[[\"BILL_AMT1\",\"BILL_AMT2\",\"BILL_AMT3\",\"BILL_AMT4\",\"BILL_AMT5\",\"BILL_AMT6\",\"PAY_AMT1\",\"PAY_AMT2\",\"PAY_AMT3\",\"PAY_AMT4\",\"PAY_AMT5\"]],np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16175532 0.21448301 0.1641436  0.15772898 0.13908899 0.16280012]\n",
      " [0.17216966 0.17142151 0.15821598 0.17170802 0.1622757  0.16420914]\n",
      " [0.18177326 0.1902991  0.15739845 0.1547137  0.14012657 0.17568892]\n",
      " [0.20669936 0.20017597 0.11092839 0.20265287 0.1336803  0.14586306]\n",
      " [0.17216966 0.17142151 0.15821598 0.17170802 0.1622757  0.16420914]]\n"
     ]
    }
   ],
   "source": [
    "# Construct input layer from borrower features\n",
    "inputs = tf.constant(borrower_features)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = tf.keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = tf.keras.layers.Dense(8,activation=\"relu\")(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = tf.keras.layers.Dense(6,activation=\"softmax\")(dense2)\n",
    "\n",
    "# Print first five predictions\n",
    "print(outputs.numpy()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Optimisers\n",
    "\n",
    "In a nerual network, we pick a point, measure the loss, and then try to move to a lower loss.        \n",
    "\n",
    "Stochastic gradient descent (SGD) optimiser       \n",
    "tf.keras.optimizers.SGD()         \n",
    "learning_rate (0.5, 0.001)       \n",
    "\n",
    "RMS propagation optimiser        \n",
    "Applies different learning rates to each feature, useful for high dimensional porblem.        \n",
    "tf.keras.optimisers.RMSprop()            \n",
    "learning_rate       \n",
    "momentum (which allows the optimizer to break through local minima.)         \n",
    "decay (low value = prevent momenum from accumulating over long period during training)       \n",
    "\n",
    "Adam (adaptive moment) optimiser        \n",
    "tf.keras.optimisers.adam()           \n",
    "learning_rate       \n",
    "beta1 (similar to decay function, low beta->decay faster)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model function\n",
    "def model(bias, weights, features=borrower_features):\n",
    "    product = tf.matmul(features,weights)\n",
    "    return tf.keras.activations.sigmoid(product+bias)\n",
    "\n",
    "def loss_function(bias, weights, targets=default, features = borrower_features):\n",
    "    predictions = model(bias, weights)\n",
    "    return tf.keras.losses.binary_crossentropy(targets, predictions) # for binary classificaiton prob, loss func\n",
    "\n",
    "#minimise the loss function with RMS propagation\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.01,momentum=0.9)\n",
    "opt.minimize(lambda: loss_function(bias,weights),var_list=[bias,weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Training a network in TensorFlow\n",
    "\n",
    "We often need to initialise hundreds or thousands of variables. (mainly for the weights)    \n",
    "We use random or algorithmic generation of initial values for gradient descent.       \n",
    "\n",
    "We can draw them from a probability distribution, suc as the normal or uniform distribution. There are also specialised options, such as the Glorot initialiser, which are designed for ML algorithm.        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low level approach to initialise a 500x500 variable\n",
    "\n",
    "# Define 500x500 random nomral variable\n",
    "weights = tf.Variable(tf.random.normal([500,500]))\n",
    "\n",
    "# Define 500x500 truncated random normal variable\n",
    "# discards very large and very small draws\n",
    "weights = tf.Variable(tf.random.truncated_normal([500,500]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high level approach by initialising a dense layer using the defaulkeras options, \n",
    "# currently the Glorot uniform initialiser.\n",
    "\n",
    "# Define a dense layer with the default initialiser\n",
    "dense = tf.keras.layers.Dense(32,activation=\"relu\")\n",
    "\n",
    "# Define a dense layer with the zeros initialiser\n",
    "dense = tf.keras.layers.Dense(32,activation=\"relu\",kernel_initialiser=\"zeros\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neual network, a solution for overfitting problem is to use dropout, an operation that will randomly drop the weights connected to certain nodes in a layer during the training process. This will force the network to develop more robust rules for classification, since it cannot rely on any particular nodes being passed to an activation function. This will tend to improve out-of-sample performance.      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout working\n",
    "\n",
    "# Define input data\n",
    "inputs = np.array(borrower_features, np.float32)\n",
    "\n",
    "# Define dense layer 1\n",
    "dense1 = tf.keras.layers.Dense(32,activation=\"relu\")(inputs)\n",
    "\n",
    "# define dense layer 2\n",
    "dense2 = tf.keras.layers.Dense(16, activation=\"relu\")(dense1)\n",
    "\n",
    "# apply dropout operation (apply a droupout layer)\n",
    "# dropout 25% of the nodes randomly\n",
    "dropout1 = tf.keras.layers.Dropout(0.25)(dense2)\n",
    "\n",
    "# Define output layer\n",
    "outputs = tf.layers.Dense(1, activation=\"sigmoid\")(dropout1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. High level API: Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 The sequential API\n",
    "\n",
    "\n",
    "Let's say we experiment with several different architetures (for the classification of the SLMNIST dataset). It has an input layer, a first hidden layer with 16 nodes, and a second hidden layer with 8 nodes. We will have 4 output nodes, since there are 4 letters in the dataset.       \n",
    "\n",
    "A good way to construct this model in Keras is to use the sequential API. It assumes that we have an input layer, some number of hidden layers, and an output layer. All of these layers are ordered one after the other in a sequence. Once we have defined an Sequential() object, we can simply stack layers on top of it sequentially using the add method.       \n",
    "\n",
    "After specifying the model, before training, we must first perform a compilation step, where we specify the optimiser and loss function.     \n",
    "\n",
    "We can also train two models **jointly** to predict the same target. We have the functional API for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1    2    3    4    5    6    7    8    9    ...  775  776  777  778  \\\n",
      "0    1  142  143  146  148  149  149  149  150  151  ...    0   15   55   63   \n",
      "1    0  141  142  144  145  147  149  150  151  152  ...  173  179  179  180   \n",
      "2    1  156  157  160  162  164  166  169  171  171  ...  181  197  195  193   \n",
      "3    3   63   26   65   86   97  106  117  123  128  ...  175  179  180  182   \n",
      "4    1  156  160  164  168  172  175  178  180  182  ...  108  107  106  110   \n",
      "\n",
      "   779  780  781  782  783  784  \n",
      "0   37   61   77   65   38   23  \n",
      "1  181  181  182  182  183  183  \n",
      "2  193  191  192  198  193  182  \n",
      "3  183  183  184  185  185  185  \n",
      "4  111  108  108  102   84   70  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "(2000, 785)\n"
     ]
    }
   ],
   "source": [
    "# Sign language MNIST dataset.\n",
    "# Used to classify 4 letters from them (image): a,b,c,d\n",
    "# each image is represented by 28x28 matrix\n",
    "slmnist_df = pd.read_csv(\"../Machine_Learning_basics/data/slmnist.csv\", header=None)\n",
    "print(slmnist_df.head())\n",
    "print(slmnist_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 12,732\n",
      "Trainable params: 12,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Define a sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define first hidden layer\n",
    "# since we will use 28x28 pixel image, reshaped into vector, we will supply 28x28 comma as the input shape\n",
    "model.add(keras.layers.Dense(16,activation=\"relu\", input_shape=(28*28,)))\n",
    "\n",
    "# Define second hidden layer\n",
    "model.add(keras.layers.Dense(8,activation=\"relu\"))\n",
    "\n",
    "# Define output layer\n",
    "model.add(keras.layers.Dense(4,activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "# this loss function is for classificaiton problems with more than 2 classes\n",
    "model.compile(\"adam\",loss=\"categorical_crossentropy\")\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the sequential API will not be sufficiently flexible to accommodate your desired model architecture and you will need to use the functional API instead. If, for instance, you want to train two models with different architectures jointly, you will need to use the functional API to do this. In this exercise, we will see how to do this. We will also use the .summary() method to examine the joint model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 12)           9420        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 8)            88          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            52          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 4)            36          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 4)            0           dense_4[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,596\n",
      "Trainable params: 9,596\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Using the functional API\n",
    "# Say we have a set of 28x28 images and a set of 10 features of metadata\n",
    "# we want to use both to predict the image's class, but restrict how they interact in our model.\n",
    "\n",
    "# Define model 1 input layer shape\n",
    "model1_inputs = tf.keras.Input(shape=(28*28,))\n",
    "\n",
    "# Define model 2 input layer shape\n",
    "model2_inputs = tf.keras.Input(shape=(10,))\n",
    "\n",
    "# next we define layer 1 and layer 2 as dense layers for model 1\n",
    "# Define layer 1 for model 1\n",
    "model1_layer1 = tf.keras.layers.Dense(12, activation=\"relu\")(model1_inputs)\n",
    "# Define layer 2 for model 1\n",
    "model1_layer2 = tf.keras.layers.Dense(4, activation=\"softmax\")(model1_layer1)\n",
    "\n",
    "# now we define layer 1 and 2 for model 2\n",
    "# Define layer 1 for model 2\n",
    "model2_layer1 = tf.keras.layers.Dense(8, activation=\"relu\")(model2_inputs)\n",
    "# Define layer 2 for model 2\n",
    "model2_layer2 = tf.keras.layers.Dense(4, activation=\"softmax\")(model2_layer1)\n",
    "\n",
    "# merge model 1 and mode 2\n",
    "merged = tf.keras.layers.add([model1_layer2, model2_layer2])\n",
    "\n",
    "# Finally we define a functional model\n",
    "model = tf.keras.Model(inputs=[model1_inputs,model2_inputs], outputs = merged)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\"adam\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training and validation with Keras\n",
    "\n",
    "Whenever we train and evaluate a model in TensorFlow, we typically use the same set of steps.      \n",
    "1) Load and clean the data      \n",
    "2) Define model and specify a architecture             \n",
    "3) Train and validate the model              \n",
    "4) Perform evaluation on the model           \n",
    "\n",
    "The fit() operation:       \n",
    "We only supplied 2 arguments to fit(): features and labels. (required)         \n",
    "Many optional arguments: batch_size, epochs, validation_split        \n",
    "\n",
    "Batch size: the number of examples in each batch is the batch size (recall we split training dataset into several batch)     \n",
    "The number of times we train on the full set of batches is called the number of epochs.       \n",
    "\n",
    "Using multiple epochs allows the model to revisit the same batches, but with different model weights and possibly optimiser parameters, since they are updated after each batch.         \n",
    "\n",
    "validation_split: divides the dataset into two parts: train set and validation set.       \n",
    "\n",
    "Finally, it is good idea to split off a test set before we begin to train and validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Define the (1st) hidden layer\n",
    "model.add(tf.keras.layers.Dense(16, activation=\"relu\", input_shape=(784,)))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(tf.keras.layers.Dense(4,activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\"adam\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "# train model\n",
    "model.fit(image_features, image_labels)\n",
    "\n",
    "# train model with validation split: 20% in the validation set\n",
    "model.fit(features, labels, epochs=10, validation_split=0.20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompile the model with accuracy metric\n",
    "model.compile(\"adam\", loss=\"categorica_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# train model with validation split\n",
    "model.fit(features,labels,epochs=10, validation_split=0.20)\n",
    "\n",
    "# evaluate the test set\n",
    "model.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we return to our sign language letter classification problem. We have 2000 images of four letters--A, B, C, and D--and we want to classify them with a high level of accuracy. We will complete all parts of the problem, including the model definition, compilation, and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare feature array\n",
    "sign_language_features = np.array(slmnist_df.iloc[:,1:],np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare target array\n",
    "slmnist_target_df = slmnist_df.iloc[:,0]\n",
    "slmnist_target_dummy = pd.get_dummies(slmnist_target_df)\n",
    "sign_language_labels = np.array(slmnist_target_dummy, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples\n",
      "Epoch 1/5\n",
      "2000/2000 [==============================] - 0s 241us/sample - loss: 2271.8006\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 0s 34us/sample - loss: 1.3864\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 0s 31us/sample - loss: 1.3864\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 0s 33us/sample - loss: 1.3864\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 0s 33us/sample - loss: 1.3864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x637ae0cd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Define a hidden layer\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(tf.keras.layers.Dense(4,activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile('SGD', loss='categorical_crossentropy')\n",
    "\n",
    "# Complete the fitting operation\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained a model to predict sign language letters in the previous exercise, but it is unclear how successful we were in doing so. In this exercise, we will try to improve upon the interpretability of our results. Since we did not use a validation split, we only observed performance improvements within the training set; however, it is unclear how much of that was due to overfitting. Furthermore, since we did not supply a metric, we only saw decreases in the loss function, which do not have any clear interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1800/1800 [==============================] - 0s 267us/sample - loss: 1.4264 - accuracy: 0.2494 - val_loss: 1.4056 - val_accuracy: 0.2200\n",
      "Epoch 2/10\n",
      "1800/1800 [==============================] - 0s 51us/sample - loss: 1.3928 - accuracy: 0.2472 - val_loss: 1.3875 - val_accuracy: 0.2150\n",
      "Epoch 3/10\n",
      "1800/1800 [==============================] - 0s 48us/sample - loss: 1.3879 - accuracy: 0.2500 - val_loss: 1.3860 - val_accuracy: 0.2500\n",
      "Epoch 4/10\n",
      "1800/1800 [==============================] - 0s 46us/sample - loss: 1.3876 - accuracy: 0.2456 - val_loss: 1.3852 - val_accuracy: 0.2500\n",
      "Epoch 5/10\n",
      "1800/1800 [==============================] - 0s 48us/sample - loss: 1.3881 - accuracy: 0.2256 - val_loss: 1.3867 - val_accuracy: 0.2800\n",
      "Epoch 6/10\n",
      "1800/1800 [==============================] - 0s 44us/sample - loss: 1.3880 - accuracy: 0.2478 - val_loss: 1.3865 - val_accuracy: 0.2800\n",
      "Epoch 7/10\n",
      "1800/1800 [==============================] - 0s 48us/sample - loss: 1.3881 - accuracy: 0.2294 - val_loss: 1.3894 - val_accuracy: 0.2150\n",
      "Epoch 8/10\n",
      "1800/1800 [==============================] - 0s 49us/sample - loss: 1.3883 - accuracy: 0.2367 - val_loss: 1.3861 - val_accuracy: 0.2800\n",
      "Epoch 9/10\n",
      "1800/1800 [==============================] - 0s 49us/sample - loss: 1.3878 - accuracy: 0.2444 - val_loss: 1.3883 - val_accuracy: 0.2150\n",
      "Epoch 10/10\n",
      "1800/1800 [==============================] - 0s 50us/sample - loss: 1.3882 - accuracy: 0.2456 - val_loss: 1.3887 - val_accuracy: 0.2150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x63783f290>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Define the first layer\n",
    "model.add(tf.keras.layers.Dense(32, activation=\"sigmoid\", input_shape=(784,)))\n",
    "\n",
    "# Add activation function to classifier\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Set the optimizer, loss function, and metrics\n",
    "model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add the number of epochs and the validation split\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we'll work with a small subset of the examples from the original sign language letters dataset. A small sample, coupled with a heavily-parameterized model, will generally lead to overfitting. This means that your model will simply memorize the class of each example, rather than identifying features that generalize to many examples.         \n",
    "\n",
    "You will detect overfitting by checking whether the validation sample loss is substantially higher than the training sample loss and whether it increases with further training. With a small sample and a high learning rate, the model will struggle to converge on an optimum. You will set a low learning rate for the optimizer, which will make it easier to identify overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "1000/1000 [==============================] - 1s 569us/sample - loss: 2296.7581 - accuracy: 0.3040 - val_loss: 61.7879 - val_accuracy: 0.3580\n",
      "Epoch 2/200\n",
      "1000/1000 [==============================] - 0s 154us/sample - loss: 8.9018 - accuracy: 0.7520 - val_loss: 0.3995 - val_accuracy: 0.9410\n",
      "Epoch 3/200\n",
      "1000/1000 [==============================] - 0s 149us/sample - loss: 0.5732 - accuracy: 0.9250 - val_loss: 0.3810 - val_accuracy: 0.9560\n",
      "Epoch 4/200\n",
      "1000/1000 [==============================] - 0s 151us/sample - loss: 0.8469 - accuracy: 0.9250 - val_loss: 0.3512 - val_accuracy: 0.9580\n",
      "Epoch 5/200\n",
      "1000/1000 [==============================] - 0s 160us/sample - loss: 0.5597 - accuracy: 0.9380 - val_loss: 3.3175 - val_accuracy: 0.7930\n",
      "Epoch 6/200\n",
      "1000/1000 [==============================] - 0s 155us/sample - loss: 0.8638 - accuracy: 0.9370 - val_loss: 0.5484 - val_accuracy: 0.9380\n",
      "Epoch 7/200\n",
      "1000/1000 [==============================] - 0s 158us/sample - loss: 0.0908 - accuracy: 0.9840 - val_loss: 0.1364 - val_accuracy: 0.9830\n",
      "Epoch 8/200\n",
      "1000/1000 [==============================] - 0s 152us/sample - loss: 0.0069 - accuracy: 0.9970 - val_loss: 0.0864 - val_accuracy: 0.9870\n",
      "Epoch 9/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 0.0126 - accuracy: 0.9960 - val_loss: 0.1159 - val_accuracy: 0.9890\n",
      "Epoch 10/200\n",
      "1000/1000 [==============================] - 0s 165us/sample - loss: 0.0324 - accuracy: 0.9880 - val_loss: 0.2167 - val_accuracy: 0.9810\n",
      "Epoch 11/200\n",
      "1000/1000 [==============================] - 0s 160us/sample - loss: 0.0414 - accuracy: 0.9940 - val_loss: 0.0650 - val_accuracy: 0.9930\n",
      "Epoch 12/200\n",
      "1000/1000 [==============================] - 0s 150us/sample - loss: 0.0044 - accuracy: 0.9980 - val_loss: 0.1806 - val_accuracy: 0.9800\n",
      "Epoch 13/200\n",
      "1000/1000 [==============================] - 0s 148us/sample - loss: 0.0018 - accuracy: 0.9990 - val_loss: 0.1187 - val_accuracy: 0.9900\n",
      "Epoch 14/200\n",
      "1000/1000 [==============================] - 0s 152us/sample - loss: 2.5742e-04 - accuracy: 1.0000 - val_loss: 0.0250 - val_accuracy: 0.9910\n",
      "Epoch 15/200\n",
      "1000/1000 [==============================] - 0s 151us/sample - loss: 0.3916 - accuracy: 0.9740 - val_loss: 0.5101 - val_accuracy: 0.9250\n",
      "Epoch 16/200\n",
      "1000/1000 [==============================] - 0s 151us/sample - loss: 3.5554 - accuracy: 0.8220 - val_loss: 9.9197 - val_accuracy: 0.5190\n",
      "Epoch 17/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 19.9637 - accuracy: 0.5970 - val_loss: 1.1472 - val_accuracy: 0.4270\n",
      "Epoch 18/200\n",
      "1000/1000 [==============================] - 0s 150us/sample - loss: 0.5019 - accuracy: 0.7960 - val_loss: 0.5038 - val_accuracy: 0.9090\n",
      "Epoch 19/200\n",
      "1000/1000 [==============================] - 0s 157us/sample - loss: 0.6469 - accuracy: 0.8510 - val_loss: 1.3358 - val_accuracy: 0.8230\n",
      "Epoch 20/200\n",
      "1000/1000 [==============================] - 0s 158us/sample - loss: 0.3409 - accuracy: 0.9210 - val_loss: 0.4967 - val_accuracy: 0.9270\n",
      "Epoch 21/200\n",
      "1000/1000 [==============================] - 0s 152us/sample - loss: 0.1829 - accuracy: 0.9470 - val_loss: 0.1267 - val_accuracy: 0.9560\n",
      "Epoch 22/200\n",
      "1000/1000 [==============================] - 0s 154us/sample - loss: 0.1187 - accuracy: 0.9640 - val_loss: 0.8719 - val_accuracy: 0.8360\n",
      "Epoch 23/200\n",
      "1000/1000 [==============================] - 0s 151us/sample - loss: 0.2825 - accuracy: 0.9280 - val_loss: 0.4069 - val_accuracy: 0.9200\n",
      "Epoch 24/200\n",
      "1000/1000 [==============================] - 0s 146us/sample - loss: 1.0633 - accuracy: 0.7030 - val_loss: 1.1614 - val_accuracy: 0.7390\n",
      "Epoch 25/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 0.7179 - accuracy: 0.8040 - val_loss: 0.3719 - val_accuracy: 0.9410\n",
      "Epoch 26/200\n",
      "1000/1000 [==============================] - 0s 154us/sample - loss: 0.2933 - accuracy: 0.9530 - val_loss: 0.2025 - val_accuracy: 0.9710\n",
      "Epoch 27/200\n",
      "1000/1000 [==============================] - 0s 154us/sample - loss: 0.2127 - accuracy: 0.9580 - val_loss: 0.1238 - val_accuracy: 0.9950\n",
      "Epoch 28/200\n",
      "1000/1000 [==============================] - 0s 161us/sample - loss: 0.1362 - accuracy: 0.9830 - val_loss: 0.1578 - val_accuracy: 0.9620\n",
      "Epoch 29/200\n",
      "1000/1000 [==============================] - 0s 150us/sample - loss: 0.1104 - accuracy: 0.9850 - val_loss: 0.0731 - val_accuracy: 0.9970\n",
      "Epoch 30/200\n",
      "1000/1000 [==============================] - 0s 161us/sample - loss: 0.0851 - accuracy: 0.9900 - val_loss: 0.0624 - val_accuracy: 0.9940\n",
      "Epoch 31/200\n",
      "1000/1000 [==============================] - 0s 179us/sample - loss: 0.0310 - accuracy: 0.9950 - val_loss: 0.0151 - val_accuracy: 0.9970\n",
      "Epoch 32/200\n",
      "1000/1000 [==============================] - 0s 164us/sample - loss: 0.0091 - accuracy: 0.9990 - val_loss: 0.0126 - val_accuracy: 0.9960\n",
      "Epoch 33/200\n",
      "1000/1000 [==============================] - 0s 154us/sample - loss: 0.0078 - accuracy: 0.9990 - val_loss: 0.0309 - val_accuracy: 0.9880\n",
      "Epoch 34/200\n",
      "1000/1000 [==============================] - 0s 154us/sample - loss: 0.0137 - accuracy: 0.9950 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "1000/1000 [==============================] - 0s 156us/sample - loss: 0.0083 - accuracy: 0.9990 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "1000/1000 [==============================] - 0s 168us/sample - loss: 0.0139 - accuracy: 0.9950 - val_loss: 0.2364 - val_accuracy: 0.9280\n",
      "Epoch 37/200\n",
      "1000/1000 [==============================] - 0s 163us/sample - loss: 0.0513 - accuracy: 0.9820 - val_loss: 0.3505 - val_accuracy: 0.8590\n",
      "Epoch 38/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 0.2856 - accuracy: 0.9210 - val_loss: 0.0699 - val_accuracy: 0.9710\n",
      "Epoch 39/200\n",
      "1000/1000 [==============================] - 0s 155us/sample - loss: 0.0902 - accuracy: 0.9640 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "1000/1000 [==============================] - 0s 163us/sample - loss: 0.9765 - accuracy: 0.7370 - val_loss: 0.0842 - val_accuracy: 0.9770\n",
      "Epoch 41/200\n",
      "1000/1000 [==============================] - 0s 164us/sample - loss: 0.3232 - accuracy: 0.9420 - val_loss: 0.0638 - val_accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "1000/1000 [==============================] - 0s 162us/sample - loss: 0.1479 - accuracy: 0.9640 - val_loss: 0.0434 - val_accuracy: 0.9980\n",
      "Epoch 43/200\n",
      "1000/1000 [==============================] - 0s 147us/sample - loss: 0.0587 - accuracy: 0.9880 - val_loss: 0.0199 - val_accuracy: 0.9980\n",
      "Epoch 44/200\n",
      "1000/1000 [==============================] - 0s 168us/sample - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "1000/1000 [==============================] - 0s 164us/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "1000/1000 [==============================] - 0s 155us/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 8.2104e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "1000/1000 [==============================] - 0s 158us/sample - loss: 8.5905e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "1000/1000 [==============================] - 0s 151us/sample - loss: 6.2324e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "1000/1000 [==============================] - 0s 151us/sample - loss: 6.1785e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 7.0637e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "1000/1000 [==============================] - 0s 150us/sample - loss: 5.5793e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "1000/1000 [==============================] - 0s 149us/sample - loss: 4.7076e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "1000/1000 [==============================] - 0s 153us/sample - loss: 4.2440e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "1000/1000 [==============================] - 0s 147us/sample - loss: 3.7035e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "1000/1000 [==============================] - 0s 151us/sample - loss: 3.4115e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "1000/1000 [==============================] - 0s 144us/sample - loss: 3.0687e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "1000/1000 [==============================] - 0s 150us/sample - loss: 3.0779e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "1000/1000 [==============================] - 0s 149us/sample - loss: 2.8386e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "1000/1000 [==============================] - 0s 152us/sample - loss: 2.6889e-04 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "1000/1000 [==============================] - 0s 158us/sample - loss: 2.8350e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "1000/1000 [==============================] - 0s 157us/sample - loss: 2.4615e-04 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "1000/1000 [==============================] - 0s 160us/sample - loss: 2.2987e-04 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "1000/1000 [==============================] - 0s 162us/sample - loss: 2.3782e-04 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "1000/1000 [==============================] - 0s 141us/sample - loss: 2.0658e-04 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "1000/1000 [==============================] - 0s 157us/sample - loss: 2.1314e-04 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "1000/1000 [==============================] - 0s 172us/sample - loss: 1.8784e-04 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "1000/1000 [==============================] - 0s 154us/sample - loss: 1.8636e-04 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 0.9980\n",
      "Epoch 70/200\n",
      "1000/1000 [==============================] - 0s 148us/sample - loss: 2.7079e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "1000/1000 [==============================] - 0s 160us/sample - loss: 2.1111e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "1000/1000 [==============================] - 0s 165us/sample - loss: 1.8700e-04 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "1000/1000 [==============================] - 0s 168us/sample - loss: 1.6246e-04 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "1000/1000 [==============================] - 0s 162us/sample - loss: 1.6735e-04 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "1000/1000 [==============================] - 0s 159us/sample - loss: 1.6004e-04 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "1000/1000 [==============================] - 0s 168us/sample - loss: 1.6267e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "1000/1000 [==============================] - 0s 171us/sample - loss: 1.5939e-04 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "1000/1000 [==============================] - 0s 164us/sample - loss: 1.4809e-04 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "1000/1000 [==============================] - 0s 168us/sample - loss: 1.4193e-04 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 0.9980\n",
      "Epoch 80/200\n",
      "1000/1000 [==============================] - 0s 166us/sample - loss: 1.4449e-04 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "1000/1000 [==============================] - 0s 168us/sample - loss: 2.8047e-04 - accuracy: 1.0000 - val_loss: 0.0074 - val_accuracy: 0.9940\n",
      "Epoch 82/200\n",
      "1000/1000 [==============================] - 0s 173us/sample - loss: 1.7383e-04 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "1000/1000 [==============================] - 0s 169us/sample - loss: 1.2015e-04 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "1000/1000 [==============================] - 0s 159us/sample - loss: 1.1825e-04 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "1000/1000 [==============================] - 0s 163us/sample - loss: 1.2407e-04 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "1000/1000 [==============================] - 0s 164us/sample - loss: 1.1744e-04 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "1000/1000 [==============================] - 0s 167us/sample - loss: 1.1114e-04 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 0.9970\n",
      "Epoch 88/200\n",
      "1000/1000 [==============================] - 0s 166us/sample - loss: 1.1985e-04 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 0.9980\n",
      "Epoch 89/200\n",
      "1000/1000 [==============================] - 0s 181us/sample - loss: 1.0159e-04 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 0.9980\n",
      "Epoch 90/200\n",
      "1000/1000 [==============================] - 0s 186us/sample - loss: 1.1135e-04 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 0.9970\n",
      "Epoch 91/200\n",
      "1000/1000 [==============================] - 0s 184us/sample - loss: 1.3946e-04 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 0.9980\n",
      "Epoch 92/200\n",
      "1000/1000 [==============================] - 0s 191us/sample - loss: 9.9752e-05 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 0.9990\n",
      "Epoch 93/200\n",
      "1000/1000 [==============================] - 0s 177us/sample - loss: 9.6813e-05 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "1000/1000 [==============================] - 0s 173us/sample - loss: 9.3227e-05 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9970\n",
      "Epoch 95/200\n",
      "1000/1000 [==============================] - 0s 183us/sample - loss: 8.0665e-05 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "1000/1000 [==============================] - 0s 178us/sample - loss: 8.1000e-05 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 0.9970\n",
      "Epoch 97/200\n",
      "1000/1000 [==============================] - 0s 172us/sample - loss: 8.5458e-05 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 0.9990\n",
      "Epoch 98/200\n",
      "1000/1000 [==============================] - 0s 161us/sample - loss: 7.8294e-05 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 0.9970\n",
      "Epoch 99/200\n",
      "1000/1000 [==============================] - 0s 169us/sample - loss: 7.6127e-05 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "1000/1000 [==============================] - 0s 175us/sample - loss: 7.8134e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "1000/1000 [==============================] - 0s 167us/sample - loss: 7.4182e-05 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 0.9980\n",
      "Epoch 102/200\n",
      "1000/1000 [==============================] - 0s 159us/sample - loss: 7.3089e-05 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "1000/1000 [==============================] - 0s 167us/sample - loss: 9.1206e-05 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "1000/1000 [==============================] - 0s 167us/sample - loss: 7.1312e-05 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 0.9980\n",
      "Epoch 105/200\n",
      "1000/1000 [==============================] - 0s 165us/sample - loss: 6.4653e-05 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "1000/1000 [==============================] - 0s 168us/sample - loss: 7.1246e-05 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "1000/1000 [==============================] - 0s 161us/sample - loss: 7.0599e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "1000/1000 [==============================] - 0s 162us/sample - loss: 6.4004e-05 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "1000/1000 [==============================] - 0s 166us/sample - loss: 6.2209e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "1000/1000 [==============================] - 0s 161us/sample - loss: 5.9962e-05 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 0.9970\n",
      "Epoch 111/200\n",
      "1000/1000 [==============================] - 0s 156us/sample - loss: 6.4076e-05 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 0.9980\n",
      "Epoch 112/200\n",
      "1000/1000 [==============================] - 0s 164us/sample - loss: 6.2613e-05 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9980\n",
      "Epoch 113/200\n",
      "1000/1000 [==============================] - 0s 171us/sample - loss: 5.9339e-05 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 0.9970\n",
      "Epoch 114/200\n",
      "1000/1000 [==============================] - 0s 175us/sample - loss: 6.3634e-05 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 0.9990\n",
      "Epoch 115/200\n",
      "1000/1000 [==============================] - 0s 175us/sample - loss: 5.7406e-05 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "1000/1000 [==============================] - 0s 178us/sample - loss: 5.3906e-05 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "1000/1000 [==============================] - 0s 178us/sample - loss: 5.3213e-05 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "1000/1000 [==============================] - 0s 171us/sample - loss: 5.0627e-05 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "1000/1000 [==============================] - 0s 211us/sample - loss: 5.2499e-05 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 0.9980\n",
      "Epoch 120/200\n",
      "1000/1000 [==============================] - 0s 173us/sample - loss: 5.1162e-05 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 0.9990\n",
      "Epoch 121/200\n",
      "1000/1000 [==============================] - 0s 183us/sample - loss: 5.2804e-05 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 0.9970\n",
      "Epoch 122/200\n",
      "1000/1000 [==============================] - 0s 196us/sample - loss: 4.8328e-05 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "1000/1000 [==============================] - 0s 182us/sample - loss: 4.7698e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "1000/1000 [==============================] - 0s 211us/sample - loss: 4.5917e-05 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 0.9990\n",
      "Epoch 125/200\n",
      "1000/1000 [==============================] - 0s 210us/sample - loss: 4.7111e-05 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 0.9980\n",
      "Epoch 126/200\n",
      "1000/1000 [==============================] - 0s 196us/sample - loss: 4.5483e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "1000/1000 [==============================] - 0s 185us/sample - loss: 4.5449e-05 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 0.9980\n",
      "Epoch 128/200\n",
      "1000/1000 [==============================] - 0s 205us/sample - loss: 4.5453e-05 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 0.9990\n",
      "Epoch 129/200\n",
      "1000/1000 [==============================] - 0s 205us/sample - loss: 4.1569e-05 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "1000/1000 [==============================] - 0s 181us/sample - loss: 3.9977e-05 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 0.9980\n",
      "Epoch 131/200\n",
      "1000/1000 [==============================] - 0s 202us/sample - loss: 4.3318e-05 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 0.9980\n",
      "Epoch 132/200\n",
      "1000/1000 [==============================] - 0s 199us/sample - loss: 3.9285e-05 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "1000/1000 [==============================] - 0s 187us/sample - loss: 3.8515e-05 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "1000/1000 [==============================] - 0s 197us/sample - loss: 3.9093e-05 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 0.9980\n",
      "Epoch 135/200\n",
      "1000/1000 [==============================] - 0s 188us/sample - loss: 3.6671e-05 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "1000/1000 [==============================] - 0s 205us/sample - loss: 3.6344e-05 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 0.9990\n",
      "Epoch 137/200\n",
      "1000/1000 [==============================] - 0s 197us/sample - loss: 3.5844e-05 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "1000/1000 [==============================] - 0s 193us/sample - loss: 3.8390e-05 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 0.9970\n",
      "Epoch 139/200\n",
      "1000/1000 [==============================] - 0s 192us/sample - loss: 3.8593e-05 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "1000/1000 [==============================] - 0s 210us/sample - loss: 3.4414e-05 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 0.9980\n",
      "Epoch 141/200\n",
      "1000/1000 [==============================] - 0s 203us/sample - loss: 3.4377e-05 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "1000/1000 [==============================] - 0s 193us/sample - loss: 3.3978e-05 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 0.9980\n",
      "Epoch 143/200\n",
      "1000/1000 [==============================] - 0s 195us/sample - loss: 3.5817e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "1000/1000 [==============================] - 0s 188us/sample - loss: 3.5491e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "1000/1000 [==============================] - 0s 193us/sample - loss: 4.8872e-05 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 0.9980\n",
      "Epoch 146/200\n",
      "1000/1000 [==============================] - 0s 196us/sample - loss: 3.3271e-05 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 0.9980\n",
      "Epoch 147/200\n",
      "1000/1000 [==============================] - 0s 180us/sample - loss: 3.2015e-05 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "1000/1000 [==============================] - 0s 178us/sample - loss: 3.0298e-05 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "1000/1000 [==============================] - 0s 185us/sample - loss: 3.0336e-05 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "1000/1000 [==============================] - 0s 190us/sample - loss: 2.9033e-05 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "1000/1000 [==============================] - 0s 191us/sample - loss: 2.8666e-05 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "1000/1000 [==============================] - 0s 188us/sample - loss: 2.8715e-05 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 0.9990\n",
      "Epoch 153/200\n",
      "1000/1000 [==============================] - 0s 192us/sample - loss: 2.9152e-05 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "1000/1000 [==============================] - 0s 191us/sample - loss: 2.6685e-05 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "1000/1000 [==============================] - 0s 190us/sample - loss: 2.6265e-05 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "1000/1000 [==============================] - 0s 190us/sample - loss: 2.5121e-05 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "1000/1000 [==============================] - 0s 199us/sample - loss: 2.7342e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "1000/1000 [==============================] - 0s 196us/sample - loss: 2.4366e-05 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "1000/1000 [==============================] - 0s 192us/sample - loss: 2.5691e-05 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "1000/1000 [==============================] - 0s 191us/sample - loss: 2.5360e-05 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 0.9990\n",
      "Epoch 161/200\n",
      "1000/1000 [==============================] - 0s 193us/sample - loss: 2.4975e-05 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "1000/1000 [==============================] - 0s 194us/sample - loss: 2.4129e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "1000/1000 [==============================] - 0s 175us/sample - loss: 2.9844e-05 - accuracy: 1.0000 - val_loss: 8.3695e-04 - val_accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "1000/1000 [==============================] - 0s 184us/sample - loss: 2.8157e-05 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 0.9970\n",
      "Epoch 165/200\n",
      "1000/1000 [==============================] - 0s 189us/sample - loss: 2.2174e-05 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "1000/1000 [==============================] - 0s 197us/sample - loss: 2.2362e-05 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "1000/1000 [==============================] - 0s 200us/sample - loss: 2.0811e-05 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "1000/1000 [==============================] - 0s 190us/sample - loss: 2.4141e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "1000/1000 [==============================] - 0s 197us/sample - loss: 2.1847e-05 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "1000/1000 [==============================] - 0s 229us/sample - loss: 2.1331e-05 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "1000/1000 [==============================] - 0s 227us/sample - loss: 1.9566e-05 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "1000/1000 [==============================] - 0s 203us/sample - loss: 1.9701e-05 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "1000/1000 [==============================] - 0s 215us/sample - loss: 1.9057e-05 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "1000/1000 [==============================] - 0s 200us/sample - loss: 1.9040e-05 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "1000/1000 [==============================] - 0s 197us/sample - loss: 2.1711e-05 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "1000/1000 [==============================] - 0s 193us/sample - loss: 1.9421e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "1000/1000 [==============================] - 0s 208us/sample - loss: 1.9252e-05 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "1000/1000 [==============================] - 0s 202us/sample - loss: 1.8027e-05 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "1000/1000 [==============================] - 0s 208us/sample - loss: 1.7460e-05 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "1000/1000 [==============================] - 0s 205us/sample - loss: 1.7946e-05 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 1.8818e-05 - accuracy: 1.0000 - val_loss: 8.5574e-04 - val_accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "1000/1000 [==============================] - 0s 239us/sample - loss: 1.8256e-05 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "1000/1000 [==============================] - 0s 210us/sample - loss: 1.6394e-05 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "1000/1000 [==============================] - 0s 204us/sample - loss: 2.0960e-05 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "1000/1000 [==============================] - 0s 210us/sample - loss: 1.5481e-05 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "1000/1000 [==============================] - 0s 220us/sample - loss: 1.6136e-05 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "1000/1000 [==============================] - 0s 200us/sample - loss: 1.5669e-05 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "1000/1000 [==============================] - 0s 192us/sample - loss: 1.5293e-05 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "1000/1000 [==============================] - 0s 191us/sample - loss: 1.5212e-05 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "1000/1000 [==============================] - 0s 202us/sample - loss: 1.4638e-05 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "1000/1000 [==============================] - 0s 209us/sample - loss: 1.4444e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "1000/1000 [==============================] - 0s 204us/sample - loss: 1.4433e-05 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "1000/1000 [==============================] - 0s 206us/sample - loss: 1.4470e-05 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "1000/1000 [==============================] - 0s 205us/sample - loss: 1.3555e-05 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "1000/1000 [==============================] - 0s 219us/sample - loss: 1.3724e-05 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "1000/1000 [==============================] - 0s 205us/sample - loss: 1.3324e-05 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "1000/1000 [==============================] - 0s 194us/sample - loss: 1.3558e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "1000/1000 [==============================] - 0s 196us/sample - loss: 1.5138e-05 - accuracy: 1.0000 - val_loss: 7.4770e-04 - val_accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "1000/1000 [==============================] - 0s 217us/sample - loss: 1.2365e-05 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "1000/1000 [==============================] - 0s 199us/sample - loss: 1.3181e-05 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x638f7e110>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Define the first layer\n",
    "model.add(tf.keras.layers.Dense(1024, activation=\"relu\", input_shape=(784,)))\n",
    "\n",
    "# Add activation function to classifier\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Finish the model compilation\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.01), \n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Complete the model fit operation\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=200, validation_split=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Training model with the Estimator API\n",
    "\n",
    "The estimator API is a high level TensorFlow submodule.     \n",
    "Relative to the core, lower-level TensorFlow APIs and the high-level Keras API, model building in the Estimator API is less flexible.       \n",
    "\n",
    "This is because it enforces a set of best practices by placing restrictions on model architecture and training.      \n",
    "The upside of using the Estimators API is that it allows for faster deployment.         \n",
    "There are many premade models that can be instantiated by setting a handful of model paramters.       \n",
    "\n",
    "\n",
    "Model specification and training:      \n",
    "1) Define feature columns, which specify the shape and type of our data.          \n",
    "2) Load and transform data within a function. The output of this function will be a dictionary obj of features and our label.         \n",
    "3) Define an estimator.         \n",
    "4) Apply train operation (then predict(), and evaluate())       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a numerica feature column\n",
    "# housing dataset\n",
    "# we supplied the dictionay key: \"size\" to the operation\n",
    "size = tf.feature_column.numeric_column(\"size\")\n",
    "\n",
    "# Define a categorical feature column\n",
    "rooms = tf.feature_column.categorical_column_with_vocabulary_list(\"rooms\",[\"1\",\"2\",\"3\",\"4\",\"5\"])\n",
    "\n",
    "# we can then merge these into a list of features columns\n",
    "feature_list = [size, rooms]\n",
    "\n",
    "# alternative, we can also define a list containing a signel vector of features\n",
    "# for SLMINST dataset\n",
    "feature_list = [tf.feature_column.numeric_column(\"image\", shape=(784,))]\n",
    "\n",
    "# Define input data function\n",
    "# just 3 example for the sake of illustration\n",
    "def input_fn():\n",
    "    #Define feature dict\n",
    "    features = {\"size\":[1340,1690,2720], \"rooms\":[1,3,4]}\n",
    "    # Define labels\n",
    "    labels = [221900,538000,180000]\n",
    "    return features, labels\n",
    "\n",
    "# Define a deep neural network regression\n",
    "model0 = tf.estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[10,6,6,3])\n",
    "\n",
    "# Train the regression model\n",
    "model0.train(input_fn,steps=20)\n",
    "\n",
    "# for classificaiton task\n",
    "model1 = tf.estimator.DNNClassifier(feature_columns = feature_list, hidden_units=[32,16,8]. n_classes=4)\n",
    "\n",
    "# Train the classifier\n",
    "model1.train(input_fn,steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we'll return to the King County housing transaction dataset from chapter 2. We will again develop and train a machine learning model to predict house prices; however, this time, we'll do it using the estimator API.      \n",
    "\n",
    "Rather than completing everything in one step, we'll break this procedure down into parts. We'll begin by defining the feature columns and loading the data. In the next exercise, we'll define and train a premade estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns for bedrooms and bathrooms\n",
    "bedrooms = tf.feature_column.numeric_column(\"bedrooms\")\n",
    "bathrooms = tf.feature_column.numeric_column(\"bathrooms\")\n",
    "\n",
    "# Define the list of feature columns\n",
    "feature_list = [bedrooms, bathrooms]\n",
    "\n",
    "def input_fn():\n",
    "\t# Define the labels\n",
    "\tlabels = np.array(housing[\"price\"])\n",
    "\t# Define the features\n",
    "\tfeatures = {'bedrooms':np.array(housing['bedrooms']), \n",
    "                'bathrooms':np.array(housing[\"bathrooms\"])}\n",
    "\treturn features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/jb/ffbhfj2s6g1ghymkbmhn1d640000gn/T/tmpx58b18_l\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/jb/ffbhfj2s6g1ghymkbmhn1d640000gn/T/tmpx58b18_l', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x63dad2e50>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/jb/ffbhfj2s6g1ghymkbmhn1d640000gn/T/tmpx58b18_l/model.ckpt.\n",
      "INFO:tensorflow:loss = 426471500000.0, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/jb/ffbhfj2s6g1ghymkbmhn1d640000gn/T/tmpx58b18_l/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 426471500000.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNRegressorV2 at 0x63d9ff4d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model and set the number of steps\n",
    "model = tf.estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\n",
    "model.train(input_fn, steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/jb/ffbhfj2s6g1ghymkbmhn1d640000gn/T/tmpyrw0m_b1\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/jb/ffbhfj2s6g1ghymkbmhn1d640000gn/T/tmpyrw0m_b1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x638ee34d0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From /Users/XavierTang/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:518: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/jb/ffbhfj2s6g1ghymkbmhn1d640000gn/T/tmpyrw0m_b1/model.ckpt.\n",
      "INFO:tensorflow:loss = 426471500000.0, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 2 into /var/folders/jb/ffbhfj2s6g1ghymkbmhn1d640000gn/T/tmpyrw0m_b1/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 426469920000.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.linear.LinearRegressorV2 at 0x63787bd10>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model and set the number of steps\n",
    "model = tf.estimator.LinearRegressor(feature_columns=feature_list)\n",
    "model.train(input_fn, steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
