{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python ML library: Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Motivation for Neural Network\n",
    "\n",
    "Imagine we need to build a model predicting how many transaction each customer will make next year.     \n",
    "We have predictive data (or features) like each customer's age, bank balance, whether they are retired etc.     \n",
    "\n",
    "Consider how a linear regression will solve this problem:    \n",
    "The linear regression embeds an assumption that the outcome, in this case how many transaction a user makes, is the sum of individual part. It starts by saying:\"what is the average?\", then the effect of each part comes in.      \n",
    "So the linear regression model is *not* identifying the interactions between these parts, and how they affect banking activity.     \n",
    "\n",
    "We can plot the prediction for retired/non-retired people (2 lines), with current bank balance on the x-axis, predicted number of transaction on the y-axis. This graph shows prediction from a model with no interactions (if these two lines are parallel, if interactions are allowed, then these 2 lines will not be parallel). In this model we simply add up the effect of the retirement status and current bank balance.     \n",
    "\n",
    "Neural networks are a powerful modeling approach that accounts for interactions like this well.     \n",
    "Deep learning uses especially powerful neural networks.     \n",
    "\n",
    "Because deep learning account for interaction so well, they perform great on most prediction problems we seen before.     \n",
    "But their ability to capture extremely complex interactions also allow them to do amazing stuff with text, images, video, source code etc.    \n",
    "\n",
    "Going back to the transaction problem, if there is an interaction between retirement status and bank balance, instead of them *separately* affect the outcome, we can calculate a function  of these variables that accounts for their interaction, and use that to predict the outcome (with other features).    \n",
    "\n",
    "In reality, we can represent interactions in neural network like:     \n",
    "Most left: Input layer (consist of predictive features, e.g.age, income)      \n",
    "Most right: Output layer (the prediction of the model, e.g. number of transaction)     \n",
    "All layers that are not the input or output layers are called *hidden layers*.     \n",
    "These are hidden because they are not something we have data about, or anything we observe directly form the world.     \n",
    "Each *node* in the hidden layer, represents an aggregation of information from our input data, and each node adds to the model's ability to capture interactions. More nodes -> more interactions we capture.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Forward propagation\n",
    "\n",
    "Forward propagation algorithm: Use data to make predictions.    \n",
    "\n",
    "Going back to the bank example where we try to predict how many transacations a user will make at our bank.    \n",
    "If we make predictions based on only the no. of children and no. of existing accounts.    \n",
    "For a customer with 2 children and 3 accounts, there will be 2 nodes (2 and 3) in the input layer.     \n",
    "The hidden layer (also 2 nodes) are connected to the input layer nodes by lines. Each line has a weight indicating how strongly that input effects the hidden node that the line ends at. These are the first set of weights: the top node of the hidden layer is conected by no. child (2) with line = 1, and no. acct (3) with line = 1. These weights are the parameter we train (the number of the lines) or change when we fit a neural network to data.      \n",
    "\n",
    "To make predictions for the top node of the hidden layer, we take the value of each node in the input layer, times it by the weight that ends at the node and sum it up. So this node will have the value of 2x1+3x1 = 5.      \n",
    "\n",
    "With each node calculated and the weight of the lines determined, we propagate through till we get the number at the output layer.    \n",
    "\n",
    "So forward propagation uses multiply and then add process. This is actually a dot product.    \n",
    "This is forward propagation for a single data point. In general, we do forward propagation for 1 data point at a time.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# forward propagation simple example\n",
    "import numpy as np\n",
    "\n",
    "# 2 children, 3 account\n",
    "input_data = np.array([2,3])\n",
    "\n",
    "# weights into each node in the hiddenlayer and to the output\n",
    "weights = {\"node_0\": np.array([1,1]),\"node_1\": np.array([-1,1]),\"output\": np.array([2,-1])}\n",
    "\n",
    "#top hidden node value\n",
    "node_0_value = (input_data * weights[\"node_0\"]).sum()\n",
    "#bottom hidden node value\n",
    "node_1_value = (input_data * weights[\"node_1\"]).sum()\n",
    "                \n",
    "hidden_layer_values = np.array([node_0_value,node_1_value])\n",
    "                \n",
    "print(hidden_layer_values)\n",
    "\n",
    "output = (hidden_layer_values*weights[\"output\"]).sum()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Activation functions\n",
    "\n",
    "Creating the multiply-add-process is only half the story for hidden layers.     \n",
    "For neural networks to achieve their maximum predictive power, we must apply *activation function* in the hidden layers.   \n",
    "\n",
    "An *activation function* allows the model to capture non-linearities. That is, if the relationships in the data are not straight-line relationships, then we will need an activation functions that captures non-linearities.    \n",
    "\n",
    "This *activation function* is something applied to the value coming into a node, which then transforms it into the value stored in that node (or the node output).        \n",
    "\n",
    "An example of an activation function is a tanh function. If applied to the top node of the hidden layer in the example, then it will be tanh(5) instead of 5.     \n",
    "\n",
    "Today, the standard in both industry and research applications is something called ReLU or Rectified Linear activation function:         \n",
    "RELU(x) = 0 if x<0, x if x>=0      \n",
    "It consist of 2 linear pieces, and can be powerful when composed together through multiple successive hidden layers.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99505475 0.99999997]\n",
      "0.9901095378334199\n"
     ]
    }
   ],
   "source": [
    "# forward propagation simple example\n",
    "# tanh as activation function\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "input_data = np.array([-1,2])\n",
    "\n",
    "# weights into each node in the hiddenlayer and to the output\n",
    "weights = {\"node_0\": np.array([3,3]),\"node_1\": np.array([1,5]),\"output\": np.array([2,-1])}\n",
    "\n",
    "#top hidden node value\n",
    "node_0_input = (input_data * weights[\"node_0\"]).sum()\n",
    "node_0_output = np.tanh(node_0_input)\n",
    "#bottom hidden node value\n",
    "node_1_input = (input_data * weights[\"node_1\"]).sum()\n",
    "node_1_output = np.tanh(node_1_input)\n",
    "                \n",
    "hidden_layer_output = np.array([node_0_output,node_1_output])\n",
    "                \n",
    "print(hidden_layer_output)\n",
    "\n",
    "output = (hidden_layer_output*weights[\"output\"]).sum()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n"
     ]
    }
   ],
   "source": [
    "# activation function: relu(x)\n",
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(0, input)\n",
    "    \n",
    "    # Return the value just calculated\n",
    "    return(output)\n",
    "\n",
    "# Calculate node 0 value: node_0_output\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "# Calculate node 1 value: node_1_output\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "# Calculate model output (do not apply relu)\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print model output\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 63, 0, 148]\n"
     ]
    }
   ],
   "source": [
    "# generate predictions for multiple data observation\n",
    "input_data = [np.array([3, 5]), np.array([ 1, -1]), np.array([0, 0]), np.array([8, 4])]\n",
    "weights = {'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define predict_with_network()\n",
    "def predict_with_network(input_data_row, weights):\n",
    "\n",
    "    # Calculate node 0 value\n",
    "    node_0_input = (input_data_row * weights[\"node_0\"]).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    # Calculate node 1 value\n",
    "    node_1_input = (input_data_row * weights[\"node_1\"]).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_layer_outputs\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    # Calculate model output\n",
    "    input_to_final_layer = (hidden_layer_outputs * weights[\"output\"]).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    # Return model output\n",
    "    return(model_output)\n",
    "\n",
    "\n",
    "# Create empty list to store prediction results\n",
    "results = []\n",
    "for input_data_row in input_data:\n",
    "    # Append prediction to results\n",
    "    results.append(predict_with_network(input_data_row,weights))\n",
    "\n",
    "# Print results\n",
    "print(results)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Deeper networks  \n",
    "\n",
    "We can forward propagate through successive layers in a similar way to what we use for a single hidden layer.      \n",
    "We first fill in the values for hidden layer one as a function of the input layer. Then apply the activation functions to fill in the values of these nodes. Then use values from the first hidden layer to fill in the second hidden layer. Then we can make a prediction based on the output of the hidden layer 2. We use the same forward propagation process, but apply the iterative process more times.     \n",
    "\n",
    "For deep learning, they internally build up representations of the patterns in the data taht are useful for making prediction. And they find increasingly complex patterns as we go through successive hidden layers of the network.      \n",
    "In this way neural networks partially replace the need for feature engineering (manually creating better predictive features).     \n",
    "Deep learning is also sometimes called representation learning, because subsequent layers build increasingly sophisticated representations of the raw data, until we get to a stage where we can make predictions.     \n",
    "\n",
    "When a nerual network tries to classify an image, the first hidden layers build up patterns or interactions that are conceptually simple. A simple interaction would like at groups of nearby pixels and find patterns like horizontal lines etc. Subsequent layers combines these information and find larger patterns.      \n",
    "\n",
    "The cool ting about deep learning is that the modeler doesn't need to specify those interactions. We never tell the model to look for horizontal lines for example. Instead when we train the model, the neural network gets weights that find the relevant patterns to make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 The need for optimisation\n",
    "\n",
    "To see the importance of model weights, we will revisit the simple example of the simple neural network above (2 nodes at input, 2 nodes at hidden layer (1 layer) and 1 output node).     \n",
    "For the moment, we won't use an activation function in this example. If our input values are 2 and 3, while the true value of the target is 13. So the closer our prediciton is to 13, the more accurate this model is for this data point.     \n",
    "If we use existing weights and perform forward propagation, our output is 9. Since the true targe value is 13, our error is 9-14=4. (error = predict - actual)      \n",
    "\n",
    "Changing any weight will change our prediction down the network. Making accurate predictions gets harder with more data points. At any set of weights, there are many values of the error, corresponding to the many points we make predictions for. We use a *loss function* to aggregate all the errors into a single measure of the model's predictive performance. A common loss function for regression task is mean-squared-error (i.e. square each error of the data points, and take the average of that as a measure of model quality). Loss function aggregate all the error into a single score. The goal is to find the weights giving the lowest value for the loss function.       \n",
    "\n",
    "We do this with an algorithm called *gradient descent*:        \n",
    "1) start at a random point     \n",
    "2) until you are somewhere flat, find the slope, and take a step downhill     \n",
    "\n",
    "If slope is positive:      \n",
    "-> going opposite slope means moving to lower numbers      \n",
    "-> subtract the slope from the current value, but too big a step might lead us astray      \n",
    "-> solution: learning rate: we multiply the slope by a small number, called the learning rate, and we change each weight by subtracting (learning rate * slope). (e.g. learning rate = 0.01)       \n",
    "\n",
    "**slope for a weight**:     \n",
    "\n",
    "weights feed from one node into another, and you always get the slope you need by multiplying 3 things:      \n",
    "1) slope of the loss function w.r.t. value at the node the weight feed into       \n",
    "(for output layer node, this is slope of mean-squred loss function w.r.t. prediction node: 2*(predicted value - actual value))      \n",
    "(for hidden layer node, this is the the slope of the loss function w.r.t. *node value*, which is the sum of slopes for every weight coming out of that node)     \n",
    "2) value of the node that feeds into our weight     \n",
    "3) slope of the activation function w.r.t. value we feed into     \n",
    "\n",
    "consider 2 nodes: 3 -- 2 --> 6 (actual = 10):     \n",
    "(ans above 3 points)     \n",
    "1) here it is actually slope of mean-squred loss function w.r.t. prediction node: 2*(predicted value - actual value) = 2 x error = 2*(6-10) = -8    \n",
    "2) 3        \n",
    "3) no activation function = 0    \n",
    "so total slope of mean squared loss function = -8x3=-24       \n",
    "\n",
    "We would now improve this weight by subtracting the learning rate times that slope i.e. 2-0.01(-24)=2.24    \n",
    "\n",
    "For multiple weights feeding to the output, we repeat this calculation separately for each weight. Then we update both weights simultaneously using their respective derivatives.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# code to calculate slopes and update weights\n",
    "weights = np.array([1,2])\n",
    "input_data = np.array([3,4])\n",
    "target = 6\n",
    "learning_rate = 0.01\n",
    "preds = (weights * input_data).sum()\n",
    "\n",
    "error = preds - target\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30 40]\n",
      "2.5\n"
     ]
    }
   ],
   "source": [
    "#slope calculation and update weights\n",
    "# first/second node value for the 1st/2nd calucalted slope\n",
    "# notice we update these two nodes simultaneously\n",
    "gradient = 2*input_data*error\n",
    "print(gradient)\n",
    "\n",
    "weights_updated = weights - learning_rate * gradient\n",
    "\n",
    "preds_updated = (weights_updated * input_data).sum()\n",
    "error_updated = preds_updated - target\n",
    "print(error_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 Back Propagation\n",
    "\n",
    "We use gradient descent to optimise weights in a simple model. We can use back propagation to calculate the slopes we need to optimise more complex deep learning models.     \n",
    "\n",
    "Just as forward propagation sends input data through the hidden layers and into the output layer, back propagation takes the error from the output layer and propagates it backward through the hidden layers, towards the input layer.    \n",
    "\n",
    "It calculates the necessary slopes sequentially from the weights closest to the prediction, through hidden layers, eventually back to the weights coming from the inputs.     \n",
    "It allows gradient descent to update all weights in neural network, by getting gradients for all weights.    \n",
    "\n",
    "We then use these slopes to update our weights.    \n",
    "\n",
    "It is important to understand the process and focus on the general structure of the algorithm.    \n",
    "\n",
    "In a big picture, we are trying to estimate the slope of the loss function w.r.t each weight in our network.      \n",
    "We always do forward propagation to make a prediction and calculate an error before we do back propagation, as we need the predicted value to get the error (which is needed to get the slope)     \n",
    "\n",
    "For ReLU activation function, the slope is 0 for x<0 and slope = 1 when x > 0.     \n",
    "\n",
    "So far we have focus on getting slopes of the loss function w.r.t. to weights. We also need to keep track of the slopes of the loss function w.r.t. to node values, because we use these slopes in our calculations of slopes at weights.    \n",
    "\n",
    "The slope of the loss function w.r.t. any node value is the sum of slopes for every weight coming out of that node.     \n",
    "\n",
    "pipepline:       \n",
    "1) start at some random set of weights     \n",
    "2) forward propagation to make a prediction     \n",
    "3) backward propagation to get slope of the loss function w.r.t. each weight, starting at the prediction node       \n",
    "4) multiply that slope by learning rate , and subtract it from the current slope    \n",
    "5) forward progagate again       \n",
    "6) keep doing it until we get to a flat part        \n",
    "\n",
    "For computation efficiency, it is common to calculate slopes on only a subset of the data (batch), for each update of the weights.     \n",
    "We then use a different batch of data to calculate the next update.        \n",
    "Once we have used all our data, we start over again at the begining of the data.     \n",
    "Each time through the training data is called an epoch.     \n",
    "If we are going through our data for the 3rd time, we say we are on the 3rd epoch.      \n",
    "\n",
    "When slopes are calculated on 1 batch at a time, rather than than on the full data, it is called *stochastic gradient descent*.       \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Keras workflow\n",
    "\n",
    "Keras workflow has 4 steps.     \n",
    "1) specify the architecture:      \n",
    "Address questions like: how many layers do we want ? How many nodes in each layer ? What activation function do we want in each layer.     \n",
    "2) compile the model     \n",
    "This specifies the loss function and some details about how optimisation works     \n",
    "3) fit the model        \n",
    "The cycle of back propagation and optimisation of model weights with our data       \n",
    "4) predict      \n",
    "predict new instances      \n",
    "\n",
    "\n",
    "**compile**:        \n",
    "It set up the network for optimisation, e.g. creating an internal function to do back-propagation efficiently.     \n",
    "The compile method has 2 importnat arg:      \n",
    "1) optimiser to use:     \n",
    "Controls the learning rate. Big impact on how quick the model finds good weights, and how good a set of weights it can find. There are a few algorithms that automatically tune the learning rate. \"Adam\" is usually a good choice. It adjust the learning rate as it does gradient descent, to ensure reasonable values throughout the weight optimisation process.     \n",
    "2) loss function:     \n",
    "\"mean_squred_error\" is the most common choice for regression problems.     \n",
    "\n",
    "**fit**:      \n",
    "Apply back-propagation and gradient descent with data to update the weights.    \n",
    "We should scale the data before fitting, so that each feature is, on average, about similar sized values.     \n",
    "Commonly we can substrat each feature by the feature mean, and divided it by it standard deviation.\n",
    "\n",
    "\n",
    "**predict**:        \n",
    "save the model after we have trained it, reload that model and make predictions with that model.      \n",
    "\n",
    "These are shown in the titanic model below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model\n",
    "# dont run, no data, just showing workflow\n",
    "#imports\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "#data loading\n",
    "#find the no. of nodes in the input layer\n",
    "# we ALWAYS need to specify how many columns are in the input when building a keras model\n",
    "# because that is the number of nodes in the input layer.\n",
    "predictors = np.loadtxt(\"example.csv\",delimiter=\",\")\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# one of the two way to build a model: sequential\n",
    "# sequential models require that each layer has weights or connections only to the one layer\n",
    "# coming directly after it in the network digram\n",
    "model = Sequential()\n",
    "#Dense layer: all the nodes in the previous layer connect to all of the nodes in the current layer\n",
    "# no. of nodes as 1st positional arg, followed by the activation function\n",
    "# in the first layer, we need to specify input shape\n",
    "# here it says input will have n_cols column and there can be any no. of rows (i.e. any no. of data point)\n",
    "# this model has 2 hidden layer and 1 output layer\n",
    "model.add(Dense(100,activation=\"relu\",input_shape=(n_cols,)))\n",
    "\n",
    "model.add(Dense(100,activation=\"relu\"))\n",
    "#output layer: 1 node\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>union</th>\n",
       "      <th>education_yrs</th>\n",
       "      <th>experience_yrs</th>\n",
       "      <th>age</th>\n",
       "      <th>female</th>\n",
       "      <th>marr</th>\n",
       "      <th>south</th>\n",
       "      <th>manufacturing</th>\n",
       "      <th>construction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.67</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\n",
       "0           5.10      0              8              21   35       1     1   \n",
       "1           4.95      0              9              42   57       1     1   \n",
       "2           6.67      0             12               1   19       0     0   \n",
       "3           4.00      0             12               4   22       0     0   \n",
       "4           7.50      0             12              17   35       0     1   \n",
       "\n",
       "   south  manufacturing  construction  \n",
       "0      0              1             0  \n",
       "1      0              1             0  \n",
       "2      0              1             0  \n",
       "3      0              0             0  \n",
       "4      0              0             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "df_wage = pd.read_csv(\"../Machine_Learning_basics/data/hourly_wages.csv\")\n",
    "df_wage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_feature_arr = df_wage.drop(\"wage_per_hour\",axis=1).values\n",
    "wage_target_arr = df_wage[\"wage_per_hour\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0829 16:35:56.767001 4800947648 deprecation_wrapper.py:119] From /Users/XavierTang/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0829 16:35:56.794703 4800947648 deprecation_wrapper.py:119] From /Users/XavierTang/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0829 16:35:56.796720 4800947648 deprecation_wrapper.py:119] From /Users/XavierTang/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model building\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = wage_feature_arr.shape[1]\n",
    "\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(50, activation=\"relu\", input_shape=(n_cols,)))\n",
    "\n",
    "# Add the second layer\n",
    "model.add(Dense(32,activation=\"relu\"))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0829 16:35:59.839310 4800947648 deprecation_wrapper.py:119] From /Users/XavierTang/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0829 16:36:00.000659 4800947648 deprecation_wrapper.py:119] From /Users/XavierTang/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0829 16:36:00.076635 4800947648 deprecation_wrapper.py:119] From /Users/XavierTang/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 121.8705\n",
      "Epoch 2/10\n",
      "534/534 [==============================] - 0s 56us/step - loss: 28.5535\n",
      "Epoch 3/10\n",
      "534/534 [==============================] - 0s 53us/step - loss: 27.1654\n",
      "Epoch 4/10\n",
      "534/534 [==============================] - 0s 51us/step - loss: 24.5848\n",
      "Epoch 5/10\n",
      "534/534 [==============================] - 0s 50us/step - loss: 23.9487\n",
      "Epoch 6/10\n",
      "534/534 [==============================] - 0s 49us/step - loss: 23.0628\n",
      "Epoch 7/10\n",
      "534/534 [==============================] - 0s 55us/step - loss: 22.7426\n",
      "Epoch 8/10\n",
      "534/534 [==============================] - 0s 51us/step - loss: 22.4230\n",
      "Epoch 9/10\n",
      "534/534 [==============================] - 0s 54us/step - loss: 22.2272\n",
      "Epoch 10/10\n",
      "534/534 [==============================] - 0s 53us/step - loss: 22.1737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1122d16d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "\n",
    "model.fit(wage_feature_arr,wage_target_arr, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep learning for classification problems\n",
    "\n",
    "Deep learning works similarly for classification, that is for prediciting outcomes from a set of discrete options.     \n",
    "For classification problem, we do a few things differently.     \n",
    "The biggest changes are:     \n",
    "1) set the loss function as \"categorical_crossentropy\" (instead of \"mean_squred_error\")     \n",
    "By far the most common loss function for classification problems (also known as LogLoss.      \n",
    "Lower score -> better      \n",
    "we can add metrics=[\"accuracy\"] to compile step for easy to understand diagnostics.     \n",
    "2) modify the last layer, as it has a separate node for each potential outcome. (if one node and binary class, then should be model.add(Dense(2,activation=\"softmax\")). Also use the \"softmax\" activation function. This ensre the prediction sum to 1, so they can interpreted like probabilities.       \n",
    "\n",
    "In general, we want to convert categoriacals in Keras to a format with a separate column for each output. (e.g. 1 for in, 0 for out). This setup is consistent with the fact that your model will have a separate node in the output for each possible class.       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>male</th>\n",
       "      <th>age_was_missing</th>\n",
       "      <th>embarked_from_cherbourg</th>\n",
       "      <th>embarked_from_queenstown</th>\n",
       "      <th>embarked_from_southampton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass   age  sibsp  parch     fare  male  age_was_missing  \\\n",
       "0         0       3  22.0      1      0   7.2500     1            False   \n",
       "1         1       1  38.0      1      0  71.2833     0            False   \n",
       "2         1       3  26.0      0      0   7.9250     0            False   \n",
       "3         1       1  35.0      1      0  53.1000     0            False   \n",
       "4         0       3  35.0      0      0   8.0500     1            False   \n",
       "\n",
       "   embarked_from_cherbourg  embarked_from_queenstown  \\\n",
       "0                        0                         0   \n",
       "1                        1                         0   \n",
       "2                        0                         0   \n",
       "3                        0                         0   \n",
       "4                        0                         0   \n",
       "\n",
       "   embarked_from_southampton  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          1  \n",
       "3                          1  \n",
       "4                          1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification problem\n",
    "# titanic dataset\n",
    "\n",
    "df_titanic = pd.read_csv(\"../Machine_Learning_basics/data/titanic_all_numeric.csv\")\n",
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0829 16:36:06.022425 4800947648 deprecation.py:323] From /Users/XavierTang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "891/891 [==============================] - 1s 590us/step - loss: 2.2089 - acc: 0.5802\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s 55us/step - loss: 0.8920 - acc: 0.6240\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s 53us/step - loss: 0.6762 - acc: 0.6723\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s 53us/step - loss: 0.6242 - acc: 0.6678\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s 54us/step - loss: 0.6188 - acc: 0.6678\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s 53us/step - loss: 0.6231 - acc: 0.6936\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s 53us/step - loss: 0.5988 - acc: 0.6947\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s 54us/step - loss: 0.5901 - acc: 0.6779\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s 53us/step - loss: 0.5943 - acc: 0.7037\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s 54us/step - loss: 0.5902 - acc: 0.6992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x647a3a1d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "titanic_feature = df_titanic.drop(\"survived\",axis=1).values\n",
    "titanic_target = to_categorical(df_titanic.survived)\n",
    "\n",
    "n_cols = 10\n",
    "\n",
    "# Set up the model\n",
    "model_titan = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model_titan.add(Dense(32,activation=\"relu\",input_shape=(n_cols,)))\n",
    "\n",
    "# Add the output layer\n",
    "model_titan.add(Dense(2,activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model_titan.compile(optimizer=\"sgd\",loss=\"categorical_crossentropy\",metrics=['accuracy'] )\n",
    "\n",
    "# Fit the model\n",
    "model_titan.fit(titanic_feature,titanic_target,epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2827639  0.7139602  0.3270516  0.6713914  0.2157818  0.31000203\n",
      " 0.5646008  0.47515666 0.38245407 0.54935557 0.5165143  0.5515343\n",
      " 0.31136832 0.37531805 0.42606467 0.37882426 0.47350386 0.4262163\n",
      " 0.48194888 0.36697975 0.41663617 0.36888194 0.41306594 0.5999066\n",
      " 0.48516622 0.41589358 0.3028043  0.82755023 0.3599452  0.29624128\n",
      " 0.4462729  0.79503936 0.3566481  0.1476609  0.68634564 0.66663814\n",
      " 0.30290273 0.30297947 0.47484908 0.46993467 0.272303   0.47890142\n",
      " 0.31874928 0.5555609  0.3785565  0.2998219  0.42991847 0.3566481\n",
      " 0.44973794 0.45695472 0.49496648 0.29715177 0.7435313  0.48910436\n",
      " 0.59155834 0.60814923 0.47122446 0.25664443 0.541987   0.49528834\n",
      " 0.29918557 0.7311017  0.7239126  0.49833217 0.52117395 0.41939643\n",
      " 0.43640032 0.32391912 0.3986137  0.2930014  0.34466943 0.51372975\n",
      " 0.65236145 0.39273584 0.6039577  0.2673332  0.29624128 0.2998219\n",
      " 0.55434346 0.41406536 0.31856614 0.2822136  0.35760373 0.6201151\n",
      " 0.4845061  0.46420857 0.56547153 0.2998219  0.8309136  0.28269422\n",
      " 0.2507819  0.30666146 0.6892084  0.375459   0.103113   0.29982185\n",
      " 0.49727747 0.6623392  0.46527097 0.41249454 0.31214368 0.29624128\n",
      " 0.6758164  0.23902474 0.21048708 0.25371426 0.35691866 0.29345363\n",
      " 0.19694142 0.52983105 0.6385448  0.46095535 0.29612657 0.42727083\n",
      " 0.46915162 0.30005753 0.07723634 0.41632292 0.8355252  0.4937486\n",
      " 0.6104472  0.2998219  0.48597738 0.46213096 0.736033   0.41256374\n",
      " 0.29344192 0.2627061  0.49176818 0.14977615 0.24296819 0.28775495\n",
      " 0.34211847 0.48910436 0.39730862 0.43179536 0.6016967  0.6477523\n",
      " 0.35680953 0.68622386 0.49109456 0.35198474 0.47206432 0.28712317\n",
      " 0.40289092 0.55405664 0.25781867 0.53855604 0.37218586 0.33365625\n",
      " 0.26146367 0.6655029  0.12346774 0.29986468 0.28292403 0.7177663\n",
      " 0.3975232  0.24469502 0.3142847  0.5268986  0.31855378 0.46926248\n",
      " 0.26366934 0.34480366 0.52579075 0.5454686  0.6913039  0.38289788\n",
      " 0.48569643 0.60148346 0.46968347 0.4646665  0.504705   0.30005753\n",
      " 0.47447327 0.31686836 0.44497597 0.5333003  0.38141683 0.11516694\n",
      " 0.56107724 0.47111982 0.47470906 0.55225813 0.5526774  0.6403866\n",
      " 0.52057916 0.454241   0.3408396  0.2074777  0.463793   0.41643348\n",
      " 0.38296992 0.53708494 0.5226995  0.8640397  0.29344192 0.17942393\n",
      " 0.3566481  0.4904694  0.28887668 0.5268986  0.19305784 0.16531457\n",
      " 0.33130398 0.5291978  0.36908484 0.41123474 0.39796278 0.44151142\n",
      " 0.2607395  0.4997421  0.2779595  0.38141683 0.2984033  0.76894003\n",
      " 0.31990394 0.42804322 0.7234348  0.35417226 0.34406638 0.39092383\n",
      " 0.14027426 0.29624128 0.7200867  0.32662836 0.3884847  0.28789845\n",
      " 0.418804   0.47667146 0.7259226  0.24509065 0.23299882 0.49037468\n",
      " 0.37272757 0.3509516  0.4356134  0.5708143  0.3884847  0.36409423\n",
      " 0.5155262  0.52057916 0.35723627 0.27518362 0.24732274 0.7349195\n",
      " 0.33058557 0.44374335 0.65376705 0.45546028 0.2815188  0.3647989\n",
      " 0.47494256 0.37808478 0.44482434 0.445503   0.72966975 0.7339063\n",
      " 0.96047246 0.49364156 0.29344192 0.49347806 0.7282435  0.14291056\n",
      " 0.3566481  0.31578672 0.47599757 0.2747792  0.87375015 0.80335313\n",
      " 0.55472815 0.16608393 0.49327534 0.4536912  0.3566481  0.7851697\n",
      " 0.20398761 0.18203558 0.45605314 0.45843902 0.09105161 0.25284103\n",
      " 0.35989577 0.32125476 0.4866972  0.25896075 0.27564454 0.29257157\n",
      " 0.33365625 0.35260504 0.7120258  0.69691545 0.381291   0.3650391\n",
      " 0.27923667 0.52117395 0.28901514 0.74878055 0.5479613  0.90842634\n",
      " 0.3566481  0.46277946 0.20187713 0.5241473  0.2998219  0.76098484\n",
      " 0.777872   0.6945537  0.43032792 0.68407387 0.71802217 0.82933426\n",
      " 0.518391   0.25371426 0.4075847  0.3253244  0.5428677  0.2743531\n",
      " 0.7951544  0.82067853 0.2779595  0.2599482  0.47762263 0.58673674\n",
      " 0.5268986  0.81844664 0.08779916 0.44844925 0.44760332 0.6554658\n",
      " 0.530816   0.4491747  0.8155643  0.46456975 0.77387667 0.29624128\n",
      " 0.6559919  0.8266371  0.16561337 0.43843588 0.5361517  0.8321143\n",
      " 0.38774523 0.39730862 0.36267912 0.4904694  0.41639572 0.51332253\n",
      " 0.49960694 0.18983668 0.31689933 0.60705686 0.33552527 0.4015248\n",
      " 0.3028043  0.28887668 0.6670453  0.4323517  0.3599452  0.3599452\n",
      " 0.3258077  0.49154022 0.36534986 0.19759215 0.42991847 0.22865823\n",
      " 0.7758637  0.36708802 0.3566481  0.6931381  0.62705326 0.29237962\n",
      " 0.32125476 0.73872787 0.4844819  0.7211911  0.3394329  0.80968624\n",
      " 0.24179125 0.3145415  0.8780868  0.5113615  0.23035505 0.6707147\n",
      " 0.29624128 0.653906   0.5357312  0.44844925 0.29296347 0.49344996\n",
      " 0.76937634 0.2970544  0.26346567 0.73762256 0.42162788 0.2902797\n",
      " 0.2905488  0.44321382 0.37585863 0.47302276 0.19232978 0.26963794\n",
      " 0.41981283 0.38155958 0.39085457 0.42814633 0.13630655 0.5217956\n",
      " 0.29657257 0.47667146 0.29624128 0.27334887 0.74786985 0.18203558\n",
      " 0.16822593 0.3636949  0.521102   0.43448314 0.38141683 0.5717292\n",
      " 0.31874928 0.29617426 0.24715005 0.4391165  0.4636323  0.2815188\n",
      " 0.4998768  0.5668295  0.29344192 0.23282407 0.49886215 0.51332253\n",
      " 0.49886593 0.31849128 0.67131114 0.68560535 0.56739414 0.450731\n",
      " 0.9254117  0.35112026 0.47805986 0.3361186  0.2747792  0.4771149\n",
      " 0.30128014 0.6401312  0.55152607 0.43459448 0.47754827 0.4580688\n",
      " 0.40195817 0.45103073 0.5045293  0.7445033  0.2998219  0.26779744\n",
      " 0.4649522  0.66251993 0.28055933 0.29344192 0.4613136  0.22136016\n",
      " 0.45770952 0.29159144 0.2998219  0.18251523 0.18203558 0.4802425\n",
      " 0.2928669  0.48827127 0.2815188  0.21074787 0.46395448 0.5021568\n",
      " 0.4064574  0.6464482  0.42814633 0.2346762  0.28403187 0.5313534\n",
      " 0.50415957 0.18203558 0.14425975 0.14861251 0.695884   0.47667146\n",
      " 0.73700476 0.48242506 0.24469502 0.49812427 0.45103073 0.28456202\n",
      " 0.46515012 0.45054835 0.30297947 0.42088193 0.75807345 0.4074272\n",
      " 0.74960154 0.27700824 0.34480366 0.36009634 0.3535776  0.28980944\n",
      " 0.67265546 0.69674355 0.42753133 0.49408075 0.39597952 0.59787637\n",
      " 0.24508014 0.29982185 0.4337785  0.7117802  0.27038932 0.4354633\n",
      " 0.39673334 0.46390143 0.48451826 0.22978102 0.7455591  0.2925716\n",
      " 0.3028043  0.7191481  0.30290273 0.18232995 0.28055933 0.82610315\n",
      " 0.19232978 0.38059527 0.53146267 0.30290273 0.3288181  0.46981016\n",
      " 0.31640497 0.5682045  0.454241   0.7686272  0.40065372 0.6681918\n",
      " 0.7355906  0.48772293 0.48895842 0.42543826 0.7738316  0.46186444\n",
      " 0.56444687 0.45727336 0.38784948 0.56038725 0.69767433 0.463569\n",
      " 0.29526776 0.29908785 0.35261798 0.47494256 0.519818   0.83113086\n",
      " 0.735572   0.4621336  0.29344192 0.1868142  0.3933248  0.2998219\n",
      " 0.36369494 0.46247232 0.3174811  0.37071344 0.30290273 0.22896482\n",
      " 0.16487168 0.62984335 0.43346596 0.3566481  0.34406638 0.39744252\n",
      " 0.4571502  0.6890126  0.5155742  0.23035513 0.5831078  0.78699595\n",
      " 0.45142663 0.60745054 0.33874807 0.6761634  0.346826   0.7577973\n",
      " 0.29612657 0.2998219  0.19891526 0.75129616 0.14539824 0.33827955\n",
      " 0.41946808 0.38304883 0.57502705 0.07298369 0.3028043  0.69285274\n",
      " 0.5684379  0.29624128 0.616955   0.17017621 0.4377588  0.35350043\n",
      " 0.24154568 0.5633232  0.6134492  0.8335767  0.40416515 0.27705064\n",
      " 0.52057916 0.29344192 0.2157818  0.6590485  0.33399743 0.4760426\n",
      " 0.5443399  0.3664976  0.38955197 0.66776586 0.3871108  0.2984097\n",
      " 0.4020418  0.47348168 0.22251794 0.6898964  0.2662807  0.29305774\n",
      " 0.45531663 0.12746732 0.5205845  0.23093684 0.50422645 0.4771149\n",
      " 0.23035513 0.4363926  0.4800955  0.4227604  0.30666146 0.6931381\n",
      " 0.50005376 0.61309606 0.48827127 0.7190632  0.3174811  0.4590187\n",
      " 0.28830254 0.33957547 0.29624128 0.54347897 0.31203845 0.3586676\n",
      " 0.35710496 0.61811244 0.29624128 0.45328552 0.40372863 0.8208061\n",
      " 0.7995532  0.19161299 0.462045   0.20027226 0.31205085 0.6382589\n",
      " 0.39730862 0.29345363 0.17483842 0.6627857  0.5452539  0.62622863\n",
      " 0.13196279 0.37826753 0.18203558 0.3244812  0.2793945  0.42989683\n",
      " 0.5630758  0.9611249  0.36657628 0.6885844  0.33315054 0.48475325\n",
      " 0.40702274 0.5929725  0.47423372 0.3493956  0.32499468 0.7912008\n",
      " 0.6415558  0.51605195 0.61309606 0.27895892 0.48161456 0.27631402\n",
      " 0.17017621 0.3562229  0.7853014  0.17315193 0.80558485 0.4314298\n",
      " 0.44674224 0.26986668 0.27004787 0.4265286  0.39045468 0.4480258\n",
      " 0.745926   0.41939643 0.6555772  0.49408075 0.64266145 0.28183824\n",
      " 0.2653556  0.3115153  0.86129063 0.45129353 0.42610568 0.22172904\n",
      " 0.575508   0.31384552 0.36888194 0.27828366 0.6241436  0.3263428\n",
      " 0.4823243  0.35632986 0.5002968  0.33960027 0.8110038  0.4925954\n",
      " 0.18203558 0.40372863 0.4037286  0.3789737  0.39352348 0.9608701\n",
      " 0.29624128 0.29624128 0.5411767  0.69569975 0.8329423  0.39633298\n",
      " 0.23619673 0.66066337 0.49190283 0.4704487  0.6036992  0.23319624\n",
      " 0.5311762  0.5103061  0.25651595 0.2858575  0.72206646 0.5147943\n",
      " 0.25161844 0.40289092 0.22136016 0.74017614 0.40065372 0.16951609\n",
      " 0.3130594  0.7798583  0.34106418 0.74585813 0.6207667  0.29209396\n",
      " 0.4617963  0.23907675 0.31643644 0.14972499 0.2367965  0.3028043\n",
      " 0.4588395  0.32386455 0.29344192 0.51454586 0.29315433 0.86870265\n",
      " 0.4332369  0.63353974 0.5352193  0.39021137 0.2544931  0.2587276\n",
      " 0.3756878  0.45685756 0.5223285  0.73375136 0.29344192 0.528097\n",
      " 0.56107724 0.5612794  0.27271056 0.3534604  0.5323084  0.30987665\n",
      " 0.24740961 0.4338771  0.36888194 0.46799225 0.6823237  0.5006154\n",
      " 0.24077423 0.23320635 0.1469581  0.3829837  0.3534604  0.66326445\n",
      " 0.26610076 0.417519   0.32288593 0.4889243  0.24168932 0.23093684\n",
      " 0.34897122 0.5928553  0.15067652 0.48521867 0.7827357  0.27665004\n",
      " 0.15110007 0.4163181  0.5166184  0.2753752  0.61309606 0.564736\n",
      " 0.29344192 0.78093654 0.46214437 0.5218339  0.30290273 0.28491583\n",
      " 0.33493292 0.744056   0.3175255  0.2998219  0.6039577  0.54782176\n",
      " 0.30835882 0.40009978 0.58018744 0.2063075  0.34480366 0.17156863\n",
      " 0.5268986  0.23115535 0.5450063  0.73308563 0.49202734 0.06947611\n",
      " 0.4937911  0.61762804 0.50347877 0.4125135  0.8468322  0.4684019\n",
      " 0.40385768 0.30290273 0.31727105 0.39717138 0.52994317 0.5610773\n",
      " 0.40051433 0.40061313 0.508474   0.63242793 0.33465457 0.48760763\n",
      " 0.2662807  0.6847871  0.26742506 0.17123176 0.4833283  0.41721252\n",
      " 0.33986923 0.3174811  0.29624128 0.77816284 0.51392555 0.22405148\n",
      " 0.42468283 0.3603119  0.2544931  0.3896365  0.39092383 0.5907158\n",
      " 0.45508996 0.5777654  0.22740667]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 32)                352       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 418\n",
      "Trainable params: 418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# save and load model after training\n",
    "from keras.models import load_model\n",
    "\n",
    "model_titan.save(\"model_titanic.h5\")\n",
    "my_model_titan = load_model(\"model_titanic.h5\")\n",
    "predictions = my_model_titan.predict(titanic_feature)\n",
    "#2 cols, 0: not survive, 1: survived, as we did to_categorical to split them into 2 cols\n",
    "probability_survived = predictions[:,1]\n",
    "\n",
    "print(probability_survived)\n",
    "print(my_model_titan.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model optimisation\n",
    "\n",
    "Need to choose model architecture and model optimisation arguments properly.    \n",
    "In practice, optimisation is a hard problem.    \n",
    "The optimal value of any one weight depends on the values of the other weights, and we are optimising many weights at once.    \n",
    "Even if the slope tells us which weights to increase/decrease, our updates may not imprive our model meaningfully.     \n",
    "A small learning rate might cause us to make such small updates to the model's weights that our model doesn't improve materially. In contrast, a very large learning rate might take us too far in the direction that seemed good.     \n",
    "Smart optimiser like Adam helps, but optimisation problem is still challenging.    \n",
    "\n",
    "The easiest way to see the effect of different learning rates is to use the simplest optimizer: Stochasitc Gradient Descent (SGD). This optimiser uses a fixed learning rate.     \n",
    "\n",
    "Even if the learning rate is well tuned, we can run into the so-called \"dying neuron\" problem:      \n",
    "This problem occurs when a neuron (node) takes a value less than 0 for all rows of our data. Recall that with ReLU activation function, any node with negative input gives 0 and a slope of 0. Because the slope is 0, the slope of any weights flowing into that nodes are also 0. So those weights don't get updated. In other words, once the node starts always getting negative input, it may continue only getting negative inputs and contributing nothing to the mode at this point.      \n",
    "\n",
    "We might consider an activation function whose slope is never exactly 0, however those types of functions were used for many years (tanh). For tanh, values outside the middle of S shape are flat -> small slope. A small but non-0 slope might work in a network with only a few hidden layers. In deep network with many layers, the repeated multiplication of small slope cause the slopes to get close to 0, which means that updates in back prop close to 0. This is called *vanishing gradient problem*. This in turn suggest that using an activation function that is not even close to flat anywhere.     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 10\n",
    "input_shape=(n_cols,)\n",
    "def get_new_model(input_shape = input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100,activation=\"relu\",input_shape=input_shape))\n",
    "    model.add(Dense(100,activation=\"relu\"))\n",
    "    model.add(Dense(2,activation=\"softmax\"))\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Testing model with learning rate: 0.000001\n",
      "\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 1s 764us/step - loss: 4.4855 - acc: 0.6162\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s 65us/step - loss: 4.4784 - acc: 0.6162\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s 66us/step - loss: 4.4711 - acc: 0.6162\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s 64us/step - loss: 4.4639 - acc: 0.6162\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s 66us/step - loss: 4.4566 - acc: 0.6162\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s 64us/step - loss: 4.4494 - acc: 0.6162\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s 68us/step - loss: 4.4421 - acc: 0.6162\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s 68us/step - loss: 4.4347 - acc: 0.6162\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s 66us/step - loss: 4.4272 - acc: 0.6162\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s 68us/step - loss: 4.4196 - acc: 0.6162\n",
      "\n",
      "\n",
      "Testing model with learning rate: 0.010000\n",
      "\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 1s 813us/step - loss: 1.5484 - acc: 0.6083\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s 67us/step - loss: 0.9226 - acc: 0.6319\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s 67us/step - loss: 0.6556 - acc: 0.6712\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s 66us/step - loss: 0.6297 - acc: 0.6768\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s 68us/step - loss: 0.6213 - acc: 0.6790\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s 67us/step - loss: 0.6236 - acc: 0.6678\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s 65us/step - loss: 0.6155 - acc: 0.6902\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s 65us/step - loss: 0.5952 - acc: 0.7071\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s 65us/step - loss: 0.6058 - acc: 0.6958\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s 67us/step - loss: 0.5951 - acc: 0.7059\n",
      "\n",
      "\n",
      "Testing model with learning rate: 1.000000\n",
      "\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 1s 816us/step - loss: 9.6377 - acc: 0.3883\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 0s 67us/step - loss: 9.9314 - acc: 0.3838\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 0s 66us/step - loss: 9.9314 - acc: 0.3838\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 0s 67us/step - loss: 9.9314 - acc: 0.3838\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 0s 68us/step - loss: 9.9314 - acc: 0.3838\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 0s 68us/step - loss: 9.9314 - acc: 0.3838\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 0s 67us/step - loss: 9.9314 - acc: 0.3838\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 0s 66us/step - loss: 9.9314 - acc: 0.3838\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 0s 68us/step - loss: 9.9314 - acc: 0.3838\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 0s 66us/step - loss: 9.9314 - acc: 0.3838\n"
     ]
    }
   ],
   "source": [
    "# Import the SGD optimizer\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Create list of learning rates: lr_to_test\n",
    "lr_to_test = [0.000001,0.01,1]\n",
    "# Loop over learning rates\n",
    "for lr in lr_to_test:\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    \n",
    "    # Build new model to test, unaffected by previous models\n",
    "    model = get_new_model()\n",
    "    \n",
    "    # Create SGD optimizer with specified learning rate: my_optimizer\n",
    "    my_optimizer = SGD(lr=lr)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=my_optimizer,loss='categorical_crossentropy',metrics=[\"accuracy\"])\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(titanic_feature,titanic_target,epochs=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model validation\n",
    "\n",
    "The model's performance on the training data is not a good indication of how it will perform on the new data. For this reason we use validation data to test model performance. Validation data is data that is explicitly held out form training, and used only to test model performance.       \n",
    "\n",
    "In practice, few people run k-fold CV on deep learning models because deep learning is typically used on large datasets.     \n",
    "So the computational expense of running k-fold CV would be large.     \n",
    "We usually trust a score from a single validation run because those validation runs are reasonably large.     \n",
    "\n",
    "In Keras, we can specify the split during the fit step: (30% of the data for validation)    \n",
    "model.fit(predictors,target,validation_split=0.3)     \n",
    "\n",
    "Our goal is to have the best validation score possible, so we should keep training while validation scores are improving, and stop when it stops improving.      \n",
    "\n",
    "**early stopping**:     \n",
    "create a early_stopping_monitor before fitting the model.     \n",
    "arg: patience = how many epochs the model can go without improving before we stop training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/20\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 2.9582 - acc: 0.5762 - val_loss: 1.0873 - val_acc: 0.7015\n",
      "Epoch 2/20\n",
      "623/623 [==============================] - 0s 66us/step - loss: 1.6847 - acc: 0.5971 - val_loss: 1.1026 - val_acc: 0.7164\n",
      "Epoch 3/20\n",
      "623/623 [==============================] - 0s 67us/step - loss: 1.0933 - acc: 0.6083 - val_loss: 0.5520 - val_acc: 0.7201\n",
      "Epoch 4/20\n",
      "623/623 [==============================] - 0s 66us/step - loss: 0.8659 - acc: 0.6629 - val_loss: 0.6306 - val_acc: 0.7201\n",
      "Epoch 5/20\n",
      "623/623 [==============================] - 0s 66us/step - loss: 0.8757 - acc: 0.6340 - val_loss: 0.8627 - val_acc: 0.6418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x64ad64f90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Set up the model\n",
    "model_titan = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model_titan.add(Dense(32,activation=\"relu\",input_shape=(n_cols,)))\n",
    "\n",
    "# Add the output layer\n",
    "model_titan.add(Dense(2,activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model_titan.compile(optimizer=\"sgd\",loss=\"categorical_crossentropy\",metrics=['accuracy'] )\n",
    "\n",
    "# Fit the model\n",
    "model_titan.fit(titanic_feature,titanic_target,validation_split=0.3,epochs=20, callbacks=[early_stopping_monitor])\n",
    "\n",
    "#looks at val_loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT RUN !\n",
    "# compare 2 models\n",
    "# Kenel restarting\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "df_titanic = pd.read_csv(\"../Machine_Learning_basics/data/titanic_all_numeric.csv\")\n",
    "#df_titanic.head()\n",
    "titanic_feature = df_titanic.drop(\"survived\",axis=1).values\n",
    "titanic_target = to_categorical(df_titanic.survived)\n",
    "\n",
    "\n",
    "n_cols = 10\n",
    "input_shape=(n_cols,)\n",
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Create the new model: model_1\n",
    "model_1 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_1.add(Dense(5, activation=\"relu\", input_shape=input_shape))\n",
    "model_1.add(Dense(5, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer\n",
    "model_1.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "# Compile model_2\n",
    "model_1.compile(optimizer=\"adam\",loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_2.add(Dense(10, activation=\"relu\", input_shape=input_shape))\n",
    "model_2.add(Dense(10, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer=\"adam\",loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model_1\n",
    "model_1_training = model_1.fit(titanic_feature,titanic_target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model_2\n",
    "model_2_training = model_2.fit(titanic_feature,titanic_target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model capacity\n",
    "\n",
    "Model capacity or network capacity is closely related to the terms overfitting and underfitting.    \n",
    "Validation score is one of the ultimate measure of a model's predictive quality.    \n",
    "Model capacity is a model's ability to capture predictive patterns in our data.     \n",
    "If we have a network, and we increase the number of nodes or neurons in a hidden layer, or add layers, that would increase model capacity.     \n",
    "\n",
    "A good workflow:    \n",
    "1) start with a simple network     \n",
    "2) get validation score    \n",
    "3) keep adding capacity until validation score is no longer improving    \n",
    "4) decrease capacity slightly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
