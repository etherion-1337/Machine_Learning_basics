{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python ML library: Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Motivation\n",
    "\n",
    "Imagine we need to build a model predicting how many transaction each customer will make next year.     \n",
    "We have predictive data (or features) like each customer's age, bank balance, whether they are retired etc.     \n",
    "\n",
    "Consider how a linear regression will solve this problem:    \n",
    "The linear regression embeds an assumption that the outcome, in this case how many transaction a user makes, is the sum of individual part. It starts by saying:\"what is the average?\", then the effect of each part comes in.      \n",
    "So the linear regression model is *not* identifying the interactions between these parts, and how they affect banking activity.     \n",
    "\n",
    "We can plot the prediction for retired/non-retired people (2 lines), with current bank balance on the x-axis, predicted number of transaction on the y-axis. This graph shows prediction from a model with no interactions (if these two lines are parallel, if interactions are allowed, then these 2 lines will not be parallel). In this model we simply add up the effect of the retirement status and current bank balance.     \n",
    "\n",
    "Neural networks are a powerful modeling approach that accounts for interactions like this well.     \n",
    "Deep learning uses especially powerful neural networks.     \n",
    "\n",
    "Because deep learning account for interaction so well, they perform great on most prediction problems we seen before.     \n",
    "But their ability to capture extremely complex interactions also allow them to do amazing stuff with text, images, video, source code etc.    \n",
    "\n",
    "Going back to the transaction problem, if there is an interaction between retirement status and bank balance, instead of them *separately* affect the outcome, we can calculate a function  of these variables that accounts for their interaction, and use that to predict the outcome (with other features).    \n",
    "\n",
    "In reality, we can represent interactions in neural network like:     \n",
    "Most left: Input layer (consist of predictive features, e.g.age, income)      \n",
    "Most right: Output layer (the prediction of the model, e.g. number of transaction)     \n",
    "All layers that are not the input or output layers are called *hidden layers*.     \n",
    "These are hidden because they are not something we have data about, or anything we observe directly form the world.     \n",
    "Each *node* in the hidden layer, represents an aggregation of information from our input data, and each node adds to the model's ability to capture interactions. More nodes -> more interactions we capture.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
