{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python ML library: Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Motivation\n",
    "\n",
    "Imagine we need to build a model predicting how many transaction each customer will make next year.     \n",
    "We have predictive data (or features) like each customer's age, bank balance, whether they are retired etc.     \n",
    "\n",
    "Consider how a linear regression will solve this problem:    \n",
    "The linear regression embeds an assumption that the outcome, in this case how many transaction a user makes, is the sum of individual part. It starts by saying:\"what is the average?\", then the effect of each part comes in.      \n",
    "So the linear regression model is *not* identifying the interactions between these parts, and how they affect banking activity.     \n",
    "\n",
    "We can plot the prediction for retired/non-retired people (2 lines), with current bank balance on the x-axis, predicted number of transaction on the y-axis. This graph shows prediction from a model with no interactions (if these two lines are parallel, if interactions are allowed, then these 2 lines will not be parallel). In this model we simply add up the effect of the retirement status and current bank balance.     \n",
    "\n",
    "Neural networks are a powerful modeling approach that accounts for interactions like this well.     \n",
    "Deep learning uses especially powerful neural networks.     \n",
    "\n",
    "Because deep learning account for interaction so well, they perform great on most prediction problems we seen before.     \n",
    "But their ability to capture extremely complex interactions also allow them to do amazing stuff with text, images, video, source code etc.    \n",
    "\n",
    "Going back to the transaction problem, if there is an interaction between retirement status and bank balance, instead of them *separately* affect the outcome, we can calculate a function  of these variables that accounts for their interaction, and use that to predict the outcome (with other features).    \n",
    "\n",
    "In reality, we can represent interactions in neural network like:     \n",
    "Most left: Input layer (consist of predictive features, e.g.age, income)      \n",
    "Most right: Output layer (the prediction of the model, e.g. number of transaction)     \n",
    "All layers that are not the input or output layers are called *hidden layers*.     \n",
    "These are hidden because they are not something we have data about, or anything we observe directly form the world.     \n",
    "Each *node* in the hidden layer, represents an aggregation of information from our input data, and each node adds to the model's ability to capture interactions. More nodes -> more interactions we capture.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Forward propagation\n",
    "\n",
    "Forward propagation algorithm: Use data to make predictions.    \n",
    "\n",
    "Going back to the bank example where we try to predict how many transacations a user will make at our bank.    \n",
    "If we make predictions based on only the no. of children and no. of existing accounts.    \n",
    "For a customer with 2 children and 3 accounts, there will be 2 nodes (2 and 3) in the input layer.     \n",
    "The hidden layer (also 2 nodes) are connected to the input layer nodes by lines. Each line has a weight indicating how strongly that input effects the hidden node that the line ends at. These are the first set of weights: the top node of the hidden layer is conected by no. child (2) with line = 1, and no. acct (3) with line = 1. These weights are the parameter we train (the number of the lines) or change when we fit a neural network to data.      \n",
    "\n",
    "To make predictions for the top node of the hidden layer, we take the value of each node in the input layer, times it by the weight that ends at the node and sum it up. So this node will have the value of 2x1+3x1 = 5.      \n",
    "\n",
    "With each node calculated and the weight of the lines determined, we propagate through till we get the number at the output layer.    \n",
    "\n",
    "So forward propagation uses multiply and then add process. This is actually a dot product.    \n",
    "This is forward propagation for a single data point. In general, we do forward propagation for 1 data point at a time.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# forward propagation simple example\n",
    "import numpy as np\n",
    "\n",
    "# 2 children, 3 account\n",
    "input_data = np.array([2,3])\n",
    "\n",
    "# weights into each node in the hiddenlayer and to the output\n",
    "weights = {\"node_0\": np.array([1,1]),\"node_1\": np.array([-1,1]),\"output\": np.array([2,-1])}\n",
    "\n",
    "#top hidden node value\n",
    "node_0_value = (input_data * weights[\"node_0\"]).sum()\n",
    "#bottom hidden node value\n",
    "node_1_value = (input_data * weights[\"node_1\"]).sum()\n",
    "                \n",
    "hidden_layer_values = np.array([node_0_value,node_1_value])\n",
    "                \n",
    "print(hidden_layer_values)\n",
    "\n",
    "output = (hidden_layer_values*weights[\"output\"]).sum()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Activation functions\n",
    "\n",
    "Creating the multiply-add-process is only half the story for hidden layers.     \n",
    "For neural networks to achieve their maximum predictive power, we must apply *activation function* in the hidden layers.   \n",
    "\n",
    "An *activation function* allows the model to capture non-linearities. That is, if the relationships in the data are not straight-line relationships, then we will need an activation functions that captures non-linearities.    \n",
    "\n",
    "This *activation function* is something applied to the value coming into a node, which then transforms it into the value stored in that node (or the node output).        \n",
    "\n",
    "An example of an activation function is a tanh function. If applied to the top node of the hidden layer in the example, then it will be tanh(5) instead of 5.     \n",
    "\n",
    "Today, the standard in both industry and research applications is something called ReLU or Rectified Linear activation function:         \n",
    "RELU(x) = 0 if x<0, x if x>=0      \n",
    "It consist of 2 linear pieces, and can be powerful when composed together through multiple successive hidden layers.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99505475 0.99999997]\n",
      "0.9901095378334199\n"
     ]
    }
   ],
   "source": [
    "# forward propagation simple example\n",
    "# tanh as activation function\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "input_data = np.array([-1,2])\n",
    "\n",
    "# weights into each node in the hiddenlayer and to the output\n",
    "weights = {\"node_0\": np.array([3,3]),\"node_1\": np.array([1,5]),\"output\": np.array([2,-1])}\n",
    "\n",
    "#top hidden node value\n",
    "node_0_input = (input_data * weights[\"node_0\"]).sum()\n",
    "node_0_output = np.tanh(node_0_input)\n",
    "#bottom hidden node value\n",
    "node_1_input = (input_data * weights[\"node_1\"]).sum()\n",
    "node_1_output = np.tanh(node_1_input)\n",
    "                \n",
    "hidden_layer_output = np.array([node_0_output,node_1_output])\n",
    "                \n",
    "print(hidden_layer_output)\n",
    "\n",
    "output = (hidden_layer_output*weights[\"output\"]).sum()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n"
     ]
    }
   ],
   "source": [
    "# activation function: relu(x)\n",
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(0, input)\n",
    "    \n",
    "    # Return the value just calculated\n",
    "    return(output)\n",
    "\n",
    "# Calculate node 0 value: node_0_output\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "# Calculate node 1 value: node_1_output\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "# Calculate model output (do not apply relu)\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print model output\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for multiple data observation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define predict_with_network()\n",
    "def predict_with_network(input_data_row, weights):\n",
    "\n",
    "    # Calculate node 0 value\n",
    "    node_0_input = (input_data_row * weights[\"node_0\"]).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    # Calculate node 1 value\n",
    "    node_1_input = (input_data_row * weights[\"node_1\"]).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_layer_outputs\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    # Calculate model output\n",
    "    input_to_final_layer = (hidden_layer_outputs * weights[\"output\"]).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    # Return model output\n",
    "    return(model_output)\n",
    "\n",
    "\n",
    "# Create empty list to store prediction results\n",
    "results = []\n",
    "for input_data_row in input_data:\n",
    "    # Append prediction to results\n",
    "    results.append(predict_with_network(input_data_row,weights))\n",
    "\n",
    "# Print results\n",
    "print(results)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
